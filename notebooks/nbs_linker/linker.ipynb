{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking articles between editions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../')\n",
    "print(os.getcwd())\n",
    "\n",
    "from utils.neural_searcher import NeuralSearcher\n",
    "from utils.paths import *\n",
    "from utils import json_helpers as jh\n",
    "from qdrant_client import QdrantClient\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, accuracy_score\n",
    "import random\n",
    "\n",
    "e1 = 'e1'\n",
    "e2 = 'e2'\n",
    "e1_json = f'{ENCYCLOPEDIAS_JSON_FOLDER}/e1'\n",
    "e2_json = f'{ENCYCLOPEDIAS_JSON_FOLDER}/e2'\n",
    "MATCH_THRESHOLD = 0.92\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Qdrant client\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "neural_searcher_e2 = NeuralSearcher(collection_name=e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To track progress\n",
    "total_entries = client.count(e1).count\n",
    "\n",
    "# Fetching entries\n",
    "vectors = []\n",
    "batch_size = 50 # This value has to be chosen carefully (Experience from testing)\n",
    "offset = None\n",
    "\n",
    "with tqdm(total=total_entries, desc=\"Fetching entries\") as pbar:\n",
    "    while(True):\n",
    "        response = client.scroll(\n",
    "            collection_name=e1, \n",
    "            with_payload=True, \n",
    "            with_vectors=True, \n",
    "            limit=batch_size,\n",
    "            offset=offset\n",
    "            )\n",
    "        records = response[0]\n",
    "        offset = response[1]\n",
    "        vectors += records\n",
    "        pbar.update(len(records))  # Update progress bar\n",
    "        if len(records) < batch_size:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create links from entry ids of e1 to e2\n",
    "# And create training data (This is on)\n",
    "text_links = []\n",
    "links_e1_to_e2 = {}\n",
    "for entry in tqdm(vectors, desc=\"Linking entries\"):\n",
    "    entry_id = entry.payload['entryid']\n",
    "    text = entry.payload['text']\n",
    "    matches = neural_searcher_e2.vector_search(entry.vector, threshold=MATCH_THRESHOLD, search_limit=1) # Can search with treshold instead\n",
    "    # Run matches through a NN perhaps\n",
    "    if matches:\n",
    "        links_e1_to_e2[entry_id] = matches[0]['entryid'] \n",
    "        text_links.append((text, matches[0]['text']))\n",
    "# Create links from entry ids of e2 to e1\n",
    "links_e2_to_e1 = {value: key for key, value in links_e1_to_e2.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make test data for deciding neural search threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_limit = 200\n",
    "data = []\n",
    "samples = random.sample(text_links, data_limit)\n",
    "\n",
    "for sample in samples:\n",
    "    item = {\n",
    "        \"e1_text\": sample[0],\n",
    "        \"e2_text\": sample[1],\n",
    "        \"valid_match\": 1\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "jh.write_items(data, 'text_links')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test threshold against test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_items = jh.read_items('text_links')\n",
    "\n",
    "test_threshold = 0.92 # 0.92 feels best, 0.89 best for Macro-f1: 0.7926\n",
    "y = []\n",
    "y_pred = []\n",
    "nbr_matches = 0\n",
    "for item in tqdm(json_items, desc='Verifying matches'):\n",
    "    e1_text = item['e1_text']\n",
    "    e2_text = item['e2_text']\n",
    "    y.append(item['valid_match'])\n",
    "    match = neural_searcher_e2.string_search(item['e1_text'], threshold=test_threshold) # String search\n",
    "    if match:\n",
    "        nbr_matches += 1\n",
    "        y_pred.append(1)\n",
    "        if item['valid_match'] == 0:\n",
    "            print(f'Expected no match: \\\"{e1_text}\\\" : \\\"{e2_text}\\\"')\n",
    "    else: \n",
    "        y_pred.append(0)\n",
    "        if item['valid_match'] == 1:\n",
    "            print(f'Expected match: \\\"{e1_text}\\\" : \\\"{e2_text}\\\"')\n",
    "\n",
    "print(f\"Made {nbr_matches} of {len(json_items)} entries\")\n",
    "print()\n",
    "print(classification_report(y, y_pred, target_names=['Not Match', 'Match']))\n",
    "print('Micro F1:', f1_score(y, y_pred, average='micro'))\n",
    "print('Macro F1', f1_score(y, y_pred, average='macro'))\n",
    "print('Accuracy', accuracy_score(y, y_pred))\n",
    "print(confusion_matrix(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make linked json files\n",
    "def write_linked_json(in_name: str, out_name: str, links_dict: dict[str, str], other_edition_key: str) -> None:\n",
    "    entries = jh.read_items(in_name)\n",
    "\n",
    "    for item in entries:\n",
    "        item[other_edition_key] = links_dict.get(item['entryid'], \"\")\n",
    "\n",
    "    jh.write_items(entries, out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_linked_json(e1_json, e1_json, links_e1_to_e2, 'e2_key')\n",
    "print(f\"Finished writing {e1_json}.json\")\n",
    "write_linked_json(e2_json, e1_json, links_e2_to_e1, 'e1_key')\n",
    "print(f\"Finished writing {e2_json}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Go through e1\n",
    "#   for every article, search for c\n",
    "\n",
    "# for each article in e2, compare text to qdrant e1, get closest matches\n",
    "# For the closest match, calculate cosine similarity, compare headword, edit-distance, \n",
    "# other features, threshold function.\n",
    "# If match, change in e1 and e2, other edition key to match"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
