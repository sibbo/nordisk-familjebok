{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for articles in WikiData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../')\n",
    "print(os.getcwd())\n",
    "\n",
    "import requests\n",
    "from utils import json_helpers as jh\n",
    "from utils.paths import *\n",
    "import regex as re\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "WIKIDATA_URL = \"https://www.wikidata.org/w/api.php\"\n",
    "WIKIPEDIA_URL = \"https://sv.wikipedia.org/w/api.php\"\n",
    "model = SentenceTransformer('KBLab/sentence-bert-swedish-cased', device='cpu')\n",
    "\n",
    "SEARCH_LIMIT = 5\n",
    "MATCH_THRESHOLD = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_wikidata(query: str, limit: int):\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"format\": \"json\",\n",
    "        \"language\": \"sv\",  # Language dictating how searches are made\n",
    "        \"uselang\": \"sv\",   # Language of item description\n",
    "        \"limit\": limit,       # Number of search results\n",
    "        \"search\": query,\n",
    "    }\n",
    "    response = requests.get(WIKIDATA_URL, params=params)\n",
    "    if 'search' not in response.json().keys():\n",
    "        return None\n",
    "    return response.json()['search']\n",
    "\n",
    "def get_wikipedia_title_from_qid(qid: str):\n",
    "    params = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"format\": \"json\",\n",
    "        # \"lang\": \"sv\",  # Language dictating how searches are made\n",
    "        \"props\": \"sitelinks\",\n",
    "        \"ids\": qid,\n",
    "        \"sitefilter\": \"svwiki\",\n",
    "        \"languages\": \"se\",\n",
    "        # \"uselang\": \"sv\",   # Language of item description\n",
    "        # \"limit\": 10,       # Number of search results\n",
    "        # \"search\": query,\n",
    "    }\n",
    "    response = requests.get(WIKIDATA_URL, params=params)\n",
    "    entity_data = response.json()['entities'].get(qid)\n",
    "\n",
    "    if not entity_data:\n",
    "        return None\n",
    "\n",
    "    wikipedia_dict = entity_data['sitelinks'].get('svwiki')\n",
    "\n",
    "    if not wikipedia_dict:\n",
    "        return None\n",
    "    \n",
    "    return wikipedia_dict['title']\n",
    "    #wikipedia_page_url = f\"https://sv.wikipedia.org/wiki/{wikipedia_dict['title'].replace(' ', '_')}\"\n",
    "    #return wikipedia_page_url\n",
    "\n",
    "def get_first_paragraph_text_wikipedia(qid: str):\n",
    "    # Extract the page title from the Wikipedia link\n",
    "    page_title = get_wikipedia_title_from_qid(qid) # TITLE \n",
    "\n",
    "    # Step 1: Get the Wikipedia page content\n",
    "    page_params = {\n",
    "        \"action\": \"parse\",\n",
    "        \"format\": \"json\",\n",
    "        \"page\": page_title,\n",
    "        \"prop\": \"text\",\n",
    "    }\n",
    "    page_response = requests.get(WIKIPEDIA_URL, params=page_params)\n",
    "    page_data = page_response.json()\n",
    "\n",
    "    if 'error' in page_data:\n",
    "        return None\n",
    "\n",
    "    # Step 2: Extract the text of the first paragraph\n",
    "    page_text = page_data['parse']['text']['*']\n",
    "\n",
    "    # Use regex to find the first paragraph within the HTML content\n",
    "    first_paragraph_match = re.search(r'<p>(.*?)</p>', page_text, re.DOTALL)\n",
    "\n",
    "    if first_paragraph_match:\n",
    "        # Remove HTML tags from the paragraph\n",
    "        first_paragraph = re.sub(r'<.*?>', '', first_paragraph_match.group(1))\n",
    "        first_paragraph = re.sub(r'\\[.*?\\]', '', first_paragraph)\n",
    "        return first_paragraph.strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def search_property(qid: str, prop: str ='P625'):\n",
    "    params = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"format\": \"json\",\n",
    "        \"languages\": \"se\",\n",
    "        \"ids\": qid,\n",
    "        \"props\": \"claims\",\n",
    "    }\n",
    "    response = requests.get(WIKIDATA_URL, params=params)\n",
    "    data = response.json()['entities'].get(qid)\n",
    "    if not data:\n",
    "        return None\n",
    "\n",
    "    prop_claim = data['claims'].get(prop)\n",
    "    if not prop_claim:\n",
    "        return None\n",
    "\n",
    "    prop_value = prop_claim[0]['mainsnak']['datavalue']['value']\n",
    "    return prop_value\n",
    "\n",
    "def get_qid(entity):\n",
    "    return entity['id']\n",
    "\n",
    "def get_description(entity):\n",
    "    return entity.get('display', {}).get('description', {}).get('value', '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with wikidata description for comparison of cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the description of each item, compute embedding with kb-sbert\n",
    "# where should we take the data from? qdrant or json or what\n",
    "def search(edition_file: str, search_limit: int, match_threshold: float):\n",
    "    items = jh.read_items(edition_file)\n",
    "\n",
    "    for e in tqdm(items):\n",
    "        if e['class'] == 0 or e['cross_ref_key'] != \"\": # Ignore cross references\n",
    "            continue\n",
    "        search_term = e['headword']\n",
    "        result = search_wikidata(search_term, search_limit)\n",
    "        if not result :\n",
    "            continue\n",
    "        vectors = []\n",
    "        for item in result:\n",
    "            qid = get_qid(item)\n",
    "            description = get_description(item)\n",
    "            wikipedia_text = get_first_paragraph_text_wikipedia(qid)\n",
    "\n",
    "            if wikipedia_text:\n",
    "                vectors.append(model.encode(wikipedia_text[:200]).tolist())\n",
    "            elif description:\n",
    "                vectors.append(model.encode(description[:200]).tolist())\n",
    "            else:\n",
    "                vectors.append([0] * 768)\n",
    "        \n",
    "        example_vector = model.encode(e['text'])\n",
    "        scores = cosine_similarity([example_vector], vectors)[0]\n",
    "        \n",
    "        if max(scores) > match_threshold: \n",
    "            best_match_index = list(scores).index(max(scores))\n",
    "            qid = get_qid(result[best_match_index])\n",
    "            coords = search_property(qid)\n",
    "            if coords != None:\n",
    "                e['latitude'] = coords['latitude']\n",
    "                e['longitude'] = coords['longitude']\n",
    "\n",
    "            e['qid'] = qid\n",
    "\n",
    "    jh.write_items(items, edition_file)\n",
    "\n",
    "    # results\n",
    "\n",
    "    # return the one with the highest cosine sim\n",
    "    # if it has P625 property (coordinate location), edit something\n",
    "    # edit json or smth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try wiki searcher against test data to decide match threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_location(entry: dict) -> bool:\n",
    "    lat = entry['correct_lat']\n",
    "    lon = entry['correct_lon']\n",
    "    pred_lat = entry['latitude']\n",
    "    pred_lon = entry['longitude']\n",
    "\n",
    "    if lat == None and lon == None and pred_lat == None and pred_lon == None:\n",
    "        return True\n",
    "    elif lat != None and lon != None and pred_lat != None and pred_lon != None:\n",
    "        lat_dif = abs(lat - pred_lat)\n",
    "        lon_dif = abs(lon - pred_lon)\n",
    "        if lat_dif < 0.5 and lon_dif < 0.5: # Number chosen intuitively\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def eval_wiki_searcher(testfile: str, edition_nbr: int, match_threshold: float):\n",
    "    search(edition_file=testfile, search_limit=5, match_threshold=match_threshold)\n",
    "    \n",
    "    e = jh.read_items(testfile)\n",
    "    nr_entries = len(e)\n",
    "    nr_correct = 0\n",
    "    for entry in e:\n",
    "        if valid_location(entry):\n",
    "            nr_correct += 1\n",
    "    \n",
    "    accuracy = nr_correct / nr_entries\n",
    "    with open(f'{WIKI_STATS_FOLDER}/e{edition_nbr}_stats_wiki.txt', 'a', encoding='utf-8') as file:\n",
    "        file.write(f'Match threshold: {match_threshold}\\n')\n",
    "        file.write(f'Accuracy score: {accuracy}\\n')\n",
    "        file.write(f'------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests. We found 0.6 to work best for both.\n",
    "# A high threshold is prefereable as after location classifier,\n",
    "# there may be non locations classified as locations.\n",
    "# MATCH_THRESHOLD = 0.6\n",
    "eval_wiki_searcher(f'{WIKI_TEST_FOLDER}/e1_test_wiki', 1, MATCH_THRESHOLD)\n",
    "eval_wiki_searcher(f'{WIKI_TEST_FOLDER}/e2_test_wiki', 2, MATCH_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link editions with wikidata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(edition_file=f'{ENCYCLOPEDIAS_JSON_FOLDER}/e1', search_limit=SEARCH_LIMIT, match_threshold=MATCH_THRESHOLD)\n",
    "search(edition_file=f'{ENCYCLOPEDIAS_JSON_FOLDER}/e2', search_limit=SEARCH_LIMIT, match_threshold=MATCH_THRESHOLD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
