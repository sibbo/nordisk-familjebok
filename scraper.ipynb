{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper\n",
    "\n",
    "This notebook is for scraping and segmenting the first and second editions of Nordisk Familjebok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import regex as re\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import joblib\n",
    "import regex_utils as ru\n",
    "import classification_utils as cu\n",
    "import mlp_classifier_utils as mcu\n",
    "\n",
    "INF = 10**9\n",
    "MAX_ENTRY_LENGTH = 200\n",
    "INDEX_SEGMENTER_THRESHOLD = 0.15 #relative edit distance threshold\n",
    "ENCYCLOPEDIAS_FOLDER = \"encyclopedias/\"\n",
    "PAGE_NUMBER_STRING = \"page_number=\"\n",
    "INDEX_STRING = \"index=\"\n",
    "ALPHABET = \"ABCDEFGHIJKLMNOPQRSTUVWXYZÅÄÖÜ\"\n",
    "\n",
    "INDEX_BEGIN = \"<b>On this page / på denna sida</b>\\n\"\n",
    "INDEX_END = \"<p>\"\n",
    "\n",
    "DELIM_BEGIN = \"<!-- mode=normal -->\"\n",
    "DELIM_END = \"<!-- NEWIMAGE2 -->\"\n",
    "\n",
    "html_entities = [\n",
    "                [\"&quot;\", \"\\\"\"],\n",
    "                [\"&rsquo;\", \"\\'\"],\n",
    "                [\"&lsquo;\", \"\\'\"],\n",
    "                [\"&ndash;\", \"-\"],\n",
    "                [\"<br>\", \"\"],\n",
    "                ['<span class=\"sp\">', \"\"],\n",
    "                ['<span class=\"overline\">', \"\"],\n",
    "                ['<span class=\"sc\">', \"\"],\n",
    "                [\"</span>\", \"\"],\n",
    "                [\"&lt;\", \"<\"],\n",
    "                [\"&gt;\", \">\"],\n",
    "                [\"&nbsp;\", \" \"],\n",
    "                [\"&amp;\", \"&\"],\n",
    "                [\"<sub>\", \"\"],\n",
    "                [\"</sub>\", \"\"],\n",
    "                [\"<sup>\", \"\"],\n",
    "                [\"</sup>\", \"\"],\n",
    "                [\"<s>\", \"\"],\n",
    "                [\"</s>\", \"\"],\n",
    "                [\"<sp>\", \"\"],\n",
    "                [\"</sp>\", \"\"],\n",
    "\n",
    "                ]\n",
    "\n",
    "\n",
    "classifier_remove_tags = [\n",
    "    [\"<b>\", \"\"],\n",
    "    [\"</b>\", \"\"],\n",
    "]\n",
    "\n",
    "word_frequencies_remove_tags = [\n",
    "    [\"<b>\", \"\"],\n",
    "    [\"</b>\", \"\"],\n",
    "    [\"<i>\", \"\"],\n",
    "    [\"</i>\", \"\"],\n",
    "]\n",
    "\n",
    "base_url = \"https://runeberg.org/nf\"\n",
    "# base_url = \"http://runeberg.org/download.pl?mode=ocrtext&work=nf\"\n",
    "\n",
    "#the ranges for the urls, they have a slightly weird format\n",
    "\n",
    "edition1_url_range = {\n",
    "    'a': \"abcdefghijklmnopqr\",\n",
    "}\n",
    "\n",
    "edition2_url_range = {\n",
    "    'b': \"abcdefghijklmnopqrst\",\n",
    "    'c': \"abcdefghijklmn\",\n",
    "}\n",
    "\n",
    "#the start and end pages for each volume\n",
    "edition1_volume_start_end = {\n",
    "    \"aa\": (9, 1579),\n",
    "    \"ab\": (9, 800),\n",
    "    \"ac\": (7, 798),\n",
    "    \"ad\": (7, 797),\n",
    "    \"ae\": (7, 798),\n",
    "    \"af\": (5, 795),\n",
    "    \"ag\": (7, 798),\n",
    "    \"ah\": (5, 799),\n",
    "    \"ai\": (7, 798),\n",
    "    \"aj\": (7, 798),\n",
    "    \"ak\": (7, 798),\n",
    "    \"al\": (7, 798),\n",
    "    \"am\": (7, 798),\n",
    "    \"an\": (7, 798),\n",
    "    \"ao\": (7, 798),\n",
    "    \"ap\": (7, 826),\n",
    "    \"aq\": (5, 804),\n",
    "    \"ar\": (3, 403),\n",
    "}\n",
    "\n",
    "#the pages where the lookup letter changes\n",
    "edition1_volume_letters = {\n",
    "    \"aa\": [([\"A\"], 1383), ([\"B\"], INF)],\n",
    "    \"ab\": [([\"B\"], 751), ([\"C\"], INF)], \n",
    "    \"ac\": [([\"C\"], 369), ([\"D\"], INF)],\n",
    "    \"ad\": [([\"D\"], 58), ([\"E\"], 464), ([\"F\"], INF)],\n",
    "    \"ae\": [([\"F\"], 380), ([\"G\"], INF)],\n",
    "    \"af\": [([\"G\"], 220), ([\"H\"], INF)],\n",
    "    \"ag\": [([\"H\"], 196), ([\"I\"], 489), ([\"J\"], 778), ([\"K\"], INF)],\n",
    "    \"ah\": [([\"K\"], INF)], \n",
    "    \"ai\": [([\"K\"], 232), ([\"L\"], INF)], \n",
    "    \"aj\": [([\"L\"], 255), ([\"M\"], INF)],\n",
    "    \"ak\": [([\"M\"], 380), ([\"N\"], INF)],\n",
    "    \"al\": [([\"N\"], 30), ([\"O\"], 277), ([\"P\"], INF)],\n",
    "    \"am\": [([\"P\"], 262), ([\"Q\"], 306), ([\"R\"], INF)],\n",
    "    \"an\": [([\"R\"], 147), ([\"S\"], INF)],\n",
    "    \"ao\": [([\"S\"], 641), ([\"T\"], INF)],\n",
    "    \"ap\": [([\"T\"], 625), ([\"U\", \"Ü\"], INF)], #special case for Ü\n",
    "    \"aq\": [([\"V\", \"W\"], INF)], #special case with W\n",
    "    \"ar\": [([\"V\", \"W\"], 35), ([\"X\"], 42), ([\"Y\"], 78), ([\"Z\"], 178), ([\"Å\"], 243), ([\"Ä\"], 277), ([\"Ö\"], INF)] #special case with W\n",
    "}\n",
    "\n",
    "#the start and end pages for each volume\n",
    "edition2_volume_start_end = {\n",
    "    \"ba\": (13, 824),\n",
    "    \"bb\": (13, 798),\n",
    "    \"bc\": (17, 808),\n",
    "    \"bd\": (17, 814),\n",
    "    \"be\": (17, 800),\n",
    "    \"bf\": (17, 814),\n",
    "    \"bg\": (17, 802),\n",
    "    \"bh\": (17, 806),\n",
    "    \"bi\": (17, 782),\n",
    "    \"bj\": (17, 804),\n",
    "    \"bk\": (17, 784),\n",
    "    \"bl\": (17, 816),\n",
    "    \"bm\": (17, 784),\n",
    "    \"bn\": (17, 784),\n",
    "    \"bo\": (17, 788),\n",
    "    \"bp\": (17, 812),\n",
    "    \"bq\": (17, 785),\n",
    "    \"br\": (17, 779),\n",
    "    \"bs\": (17, 820),\n",
    "    \"bt\": (17, 796),\n",
    "    \"ca\": (17, 812),\n",
    "    \"cb\": (17, 778),\n",
    "    \"cc\": (17, 817),\n",
    "    \"cd\": (17, 784),\n",
    "    \"ce\": (17, 794),\n",
    "    \"cf\": (17, 820),\n",
    "    \"cg\": (17, 806),\n",
    "    \"ch\": (17, 688),\n",
    "    \"ci\": (17, 458),\n",
    "    \"cj\": (17, 719),\n",
    "    \"ck\": (17, 688),\n",
    "    \"cl\": (17, 686),\n",
    "    \"cm\": (17, 685),\n",
    "    \"cn\": (17, 180),\n",
    "}\n",
    "\n",
    "#the pages where the lookup letter changes\n",
    "edition2_volume_letters = {\n",
    "    \"ba\": [([\"A\"], INF)],\n",
    "    \"bb\": [([\"A\"], 310), ([\"B\"], INF)],\n",
    "    \"bc\": [([\"B\"], INF)],\n",
    "    \"bd\": [([\"B\"], 519), ([\"C\"], INF)],\n",
    "    \"be\": [([\"C\"], 558), ([\"D\"], INF)],\n",
    "    \"bf\": [([\"D\"], 678), ([\"E\"], INF)],\n",
    "    \"bg\": [([\"E\"], 651), ([\"F\"], INF)],\n",
    "    \"bh\": [([\"F\"], INF)],\n",
    "    \"bi\": [([\"F\"], 281), ([\"G\"], INF)],\n",
    "    \"bj\": [([\"G\"], 506), ([\"H\"], INF)],\n",
    "    \"bk\": [([\"H\"], INF)],\n",
    "    \"bl\": [([\"H\"], 180), ([\"I\"], 611), ([\"J\"], INF)],\n",
    "    \"bm\": [([\"J\"], 275), ([\"K\"], INF)],\n",
    "    \"bn\": [([\"K\"], INF)],\n",
    "    \"bo\": [([\"K\"], 385), ([\"L\"], INF)],\n",
    "    \"bp\": [([\"L\"],  INF)],\n",
    "    \"bq\": [([\"L\"], 180), ([\"M\"], INF)],\n",
    "    \"br\": [([\"M\"], INF)],\n",
    "    \"bs\": [([\"M\"], 213), ([\"N\"], INF)],\n",
    "    \"bt\": [([\"N\"], 213), ([\"O\"], 641), ([\"P\"], INF)],\n",
    "    \"ca\": [([\"P\"], INF)],\n",
    "    \"cb\": [([\"P\"], 385), ([\"Q\"], 418), ([\"R\"], INF)],\n",
    "    \"cc\": [([\"R\"], INF)],\n",
    "    \"cd\": [([\"R\"], 136), ([\"S\"], INF)],\n",
    "    \"ce\": [([\"S\"], INF)],\n",
    "    \"cf\": [([\"S\"], INF)],\n",
    "    \"cg\": [([\"S\"], INF)],\n",
    "    \"ch\": [([\"S\"], 138), ([\"T\"], INF)],\n",
    "    \"ci\": [([\"T\"], INF)],\n",
    "    \"cj\": [([\"T\"], 441), ([\"U\"], INF)],\n",
    "    \"ck\": [([\"U\"], 116), ([\"V\"], INF)],\n",
    "    \"cl\": [([\"V\", \"W\"], INF)], #Special case with W\n",
    "    \"cm\": [([\"V\", \"W\"], 281), ([\"X\"], 291), ([\"Y\"], 357), ([\"Z\"], 488), ([\"Å\"], 619), ([\"Ä\"], INF)], #special case with W?\n",
    "    \"cn\": [([\"Ö\"], INF)]\n",
    "}\n",
    "\n",
    "edition1_volumes = edition1_volume_start_end.keys()\n",
    "edition2_volumes = edition2_volume_start_end.keys()\n",
    "\n",
    "#folder to save the .txt files in\n",
    "folder_edition1 = ENCYCLOPEDIAS_FOLDER + \"first/\"\n",
    "folder_edition2 = ENCYCLOPEDIAS_FOLDER + \"second/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_substring_between_delimiters(s: str, start: str, end: str) -> str:\n",
    "    start_index = s.find(start)\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "    start_index += len(start)\n",
    "    end_index = s.find(end, start_index)\n",
    "    if end_index == -1:\n",
    "        return None\n",
    "\n",
    "    return s[start_index: end_index]\n",
    "\n",
    "def clean_html_markup(s: str, html_entities: list) -> str:\n",
    "    res = s\n",
    "    for pair in html_entities:\n",
    "        res = res.replace(pair[0], pair[1])\n",
    "    return res\n",
    "\n",
    "def remove_single_newline(s: str) -> str:\n",
    "    return re.sub(r'(?<!\\n)\\n(?!\\n)|(\\n+)(?=\\n)', ' ', s)\n",
    "\n",
    "def scrape_page_text_and_index(url: str) -> tuple[str, str]:\n",
    "    try:\n",
    "        page = urlopen(url)\n",
    "    except:\n",
    "        return None, None\n",
    "    html = page.read().decode(\"utf-8\")\n",
    "    index = get_substring_between_delimiters(html, INDEX_BEGIN, INDEX_END)\n",
    "    html = get_substring_between_delimiters(html, DELIM_BEGIN, DELIM_END)\n",
    "    if not index == None:\n",
    "        index = clean_html_markup(index, html_entities)\n",
    "        index = remove_single_newline(index)\n",
    "    if not html == None:\n",
    "        html = clean_html_markup(html, html_entities)\n",
    "        html = remove_single_newline(html)\n",
    "    return html, index\n",
    "\n",
    "def create_url(partial_url: str, i: int) -> str:\n",
    "    return partial_url + f\"{i:04d}\" + \".html\"\n",
    "\n",
    "def scrape_volume(base_url: str, volume_start_number: int, volume_end_number: int = 9999999) -> str:\n",
    "    i = volume_start_number\n",
    "    volume_str: str = \"\"\n",
    "    while(i <= volume_end_number):\n",
    "        url = create_url(base_url, i)\n",
    "        text, index = scrape_page_text_and_index(url)\n",
    "        if text == None or index == None:\n",
    "            i += 1\n",
    "            continue\n",
    "        volume_str += PAGE_NUMBER_STRING + str(i) + \", \"\n",
    "        volume_str += INDEX_STRING + index + \"\\n\"\n",
    "        volume_str += text\n",
    "        print(f\"i = {i}: {volume_str[-10:]}\")\n",
    "        i += 1\n",
    "    return volume_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the first edition (1800-tals utgåvan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SCRAPING ---\n",
    "first_letter_list = 'a'\n",
    "for second_letter in edition1_url_range[first_letter_list]:\n",
    "    volume_index = first_letter_list + second_letter\n",
    "    f = open(folder_edition1 + volume_index + \".txt\", \"w\", encoding='utf-8')\n",
    "    volume_url = base_url + volume_index + \"/\"\n",
    "    print(volume_url)\n",
    "    f.write(scrape_volume(volume_url, \n",
    "        volume_start_number=edition1_volume_start_end[volume_index][0], \n",
    "        volume_end_number=edition1_volume_start_end[volume_index][1])) \n",
    "    print(f\"volume index: {volume_index}\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the second edition (ugglan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SCRAPING ---\n",
    "for first_letter_list in ('b', 'c'):\n",
    "    for second_letter in edition2_url_range[first_letter_list]:\n",
    "        volume_index = first_letter_list + second_letter\n",
    "        f = open(folder_edition2 + volume_index + \".txt\", \"w\", encoding='utf-8')\n",
    "        volume_url = base_url + volume_index + \"/\"\n",
    "        print(volume_url)\n",
    "        f.write(scrape_volume(volume_url, \n",
    "            volume_start_number=edition2_volume_start_end[volume_index][0], \n",
    "            volume_end_number=edition2_volume_start_end[volume_index][1])) \n",
    "        print(f\"volume index: {volume_index}\")\n",
    "        f.close()\n",
    "\n",
    "# volume_index = 'bo'\n",
    "# f = open(folder + volume_index + \".txt\", \"w\", encoding='utf-8')\n",
    "# print(f\"volume index: {volume_index}\")\n",
    "# volume_url = base_url + volume_index + \"/\"\n",
    "# print(volume_url)\n",
    "# if volume_index in [\"ba\", \"bb\"]:\n",
    "#     text = scrape_volume(volume_url, volume_start_number_ba_bb)\n",
    "#     print(text)\n",
    "#     f.write(text)\n",
    "# else:\n",
    "#     f.write(scrape_volume(volume_url, volume_start_number))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for entry classification using the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_string_to_list(index: str) -> list[str]:\n",
    "    return [query.strip() for query in index.split(\" - \")][1:]\n",
    "\n",
    "def clean_text_and_index(text_word: str, index_word: str) -> str:\n",
    "    # Clean text_word, e.g., remove italic tags, [...].\n",
    "    tags = [\n",
    "                [\"<b>\", \"\"],\n",
    "                [\"</b>\", \"\"],\n",
    "                [\"<i>\", \"\"],\n",
    "                [\"</i>\", \"\"],\n",
    "                ]\n",
    "    text_word = clean_html_markup(text_word, tags)\n",
    "\n",
    "    #if not '[' in index_word:\n",
    "    text_word = re.sub(r'\\s*\\[(.*?)\\]', '', text_word)\n",
    "    index_word = re.sub(r'\\s*\\[(.*?)\\]', '', index_word)    \n",
    "    #if not '(' in index_word:\n",
    "    text_word = re.sub(r'\\s*\\((.*?)\\)', '', text_word)\n",
    "    index_word = re.sub(r'\\s*\\((.*?)\\)', '', index_word)\n",
    "\n",
    "    return text_word, index_word\n",
    "\n",
    "def edit_distance(text_word: str, index_word: str) -> int:\n",
    "    \n",
    "    #Initializing distance matrix\n",
    "    distances = np.zeros((len(text_word) + 1, len(index_word) + 1))\n",
    "    for t1 in range(len(text_word) + 1):\n",
    "        distances[t1][0] = t1\n",
    "    for t2 in range(len(index_word) + 1):\n",
    "        distances[0][t2] = t2\n",
    "\n",
    "    # Computation\n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    \n",
    "    for t1 in range(1, len(text_word) + 1):\n",
    "        for t2 in range(1, len(index_word) + 1):\n",
    "            if (text_word[t1-1] == index_word[t2-1]):\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "            else:\n",
    "                a = distances[t1][t2 - 1]\n",
    "                b = distances[t1 - 1][t2]\n",
    "                c = distances[t1 - 1][t2 - 1]\n",
    "                \n",
    "                if (a <= b and a <= c):\n",
    "                    distances[t1][t2] = a + 1\n",
    "                elif (b <= a and b <= c):\n",
    "                    distances[t1][t2] = b + 1\n",
    "                else:\n",
    "                    distances[t1][t2] = c + 1\n",
    "\n",
    "    return distances[len(text_word)][len(index_word)]\n",
    "\n",
    "def print_distances(distances, token1_length, token2_length):\n",
    "    for t1 in range(token1_length + 1):\n",
    "        for t2 in range(token2_length + 1):\n",
    "            print(int(distances[t1][t2]), end=\" \")\n",
    "        print()\n",
    "\n",
    "def relative_edit_distance(text_word: str, index_word: str) -> float:\n",
    "    return edit_distance(text_word, index_word) / len(index_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "First checks if a word is bold, in which case we say it's an entry\n",
    "\n",
    "If not, we check if we can find it in the index\n",
    "\n",
    "Otherwise, we use a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier model\n",
    "# Load the pre-trained logistic regression model from disk\n",
    "model = joblib.load('mlp_model.pkl')\n",
    "\n",
    "#TODO: ta bort jättefult så här får man inte göra\n",
    "# frequencies, max_frequency = cu.get_word_frequencies()\n",
    "\n",
    "#TEMPORARY\n",
    "edition_nbr = 1\n",
    "\n",
    "data = []\n",
    "entry_nbr = 0\n",
    "page_entry_nbr = 0\n",
    "page_nbr = 0\n",
    "index = []\n",
    "is_entry = False\n",
    "bold_hits = 0\n",
    "index_hits = 0\n",
    "classifier_hits = 0\n",
    "first_letter_list: list[str] = []\n",
    "first_letter_boundary = 0\n",
    "volume_letters_index = -1\n",
    "classifier_type = 0 # 0 = bold, 1 = index, 2 = neural network/logistic regression classifier\n",
    "for volume in edition1_volumes:\n",
    "    first_letter_boundary = 0\n",
    "    volume_letters_index = -1\n",
    "    page_nbr = 0\n",
    "    with open(folder_edition1 + f\"{volume}.txt\", \"r\", encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            entryid = f\"e{edition_nbr}_{entry_nbr}_{volume}_{page_nbr}_{page_entry_nbr}\"\n",
    "            pagenbr_matches = re.search(r'page_number=(\\d+)', line)\n",
    "            if pagenbr_matches:\n",
    "                page_nbr = int(pagenbr_matches.group(1))\n",
    "                if page_nbr > first_letter_boundary:\n",
    "                    volume_letters_index += 1\n",
    "                    first_letter_list = edition1_volume_letters[volume][volume_letters_index][0]\n",
    "                    first_letter_boundary = edition1_volume_letters[volume][volume_letters_index][1]\n",
    "                page_entry_nbr = 0\n",
    "                start_index = line.find(INDEX_STRING)\n",
    "                index = index_string_to_list(line[start_index + len(INDEX_STRING):]) #Identical for lines on same page\n",
    "                index = sorted(index, key=len, reverse=True) # To solve problem (Arm, Armadillo)\n",
    "                print(f\"page_nbr: {page_nbr}: \", index)\n",
    "            else:\n",
    "                line = line.rstrip()[:MAX_ENTRY_LENGTH] # :200\n",
    "                if line:\n",
    "                    # --- BOLD MATCHING ---\n",
    "                    if line.startswith(tuple([f\"<b>{l}\" for l in first_letter_list])):\n",
    "                        is_entry = True\n",
    "                        classifier_type = 0\n",
    "                        headword = \"\"\n",
    "                        matches = re.findall(r'<b>(.*?)<\\/b>', line)\n",
    "                        bold_hits += 1\n",
    "                        if matches:\n",
    "                            headword = re.sub(r'[,.]$', '', matches[0])\n",
    "\n",
    "                    elif line and line[0] in first_letter_list and (len(line) > 40) and not (len(line) < 75 and line.find(\". Se \") != -1): # Removing special case\n",
    "\n",
    "                        # --- INDEX MATCHING ---    \n",
    "                        if index and not any(\"...\" in s for s in index): # index \n",
    "                            smallest_dist = INF\n",
    "                            smallest_index = -1\n",
    "                            for i, index_word in enumerate(index):\n",
    "                                temp_line, temp_index = clean_text_and_index(line, index_word)\n",
    "                                if relative_edit_distance(temp_line[:len(temp_index)], temp_index) < INDEX_SEGMENTER_THRESHOLD: \n",
    "                                    headword = index_word\n",
    "                                    is_entry = True\n",
    "                                    classifier_type = 1\n",
    "                                    index.pop(i)\n",
    "                                    index_hits += 1\n",
    "                                    # print(f\"Line = {line[:20]}, Index_word: {headword}\")\n",
    "                                    break\n",
    "                            if not is_entry:\n",
    "                                print(f\"NOT FOUND IN INDEX: {line[:20]}\")\n",
    "                        \n",
    "                        # --- CLASSIFIER MATCHING ---\n",
    "                        else: \n",
    "                            x = mcu.transform_sentence(line)\n",
    "                            if model.predict(x)[0] == 1:\n",
    "                                is_entry = True\n",
    "                                classifier_type = 2\n",
    "                                classifier_hits += 1\n",
    "                                headword = ru.words_in_line(line)[0]\n",
    "                            else:\n",
    "                                print(f\"NON-ENTRY ACCORDING TO CLASSIFIER: {line[:20]}\")\n",
    "\n",
    "                        \n",
    "                    if is_entry:\n",
    "                        item = {\n",
    "                            \"headword\": headword,\n",
    "                            \"entryid\": entryid,\n",
    "                            \"text\": line,\n",
    "                            \"classifier_type\": classifier_type,\n",
    "                            \"type\": 0,\n",
    "                            \"qid\": \"0\",\n",
    "                            \"second_edition_key\": \"\",\n",
    "                            \"fourth_edition_key\": \"\"\n",
    "                        }\n",
    "                        data.append(item)\n",
    "                        page_entry_nbr += 1\n",
    "                        entry_nbr += 1\n",
    "                        is_entry = False\n",
    "\n",
    "        \n",
    "with open(\"e1.json\", 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Bold hits: {bold_hits}\")\n",
    "print(f\"Index hits: {index_hits}\")\n",
    "print(f\"Classifier hits: {classifier_hits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#volume = open(folder_edition2 + \"bo.txt\", \"r\", encoding='utf-8')\n",
    "\n",
    "volumes = edition2_volume_start_end.keys()\n",
    "\n",
    "occurences = {}\n",
    "\n",
    "total_words = 0\n",
    "\n",
    "page_nbr = 0\n",
    "is_entry = False\n",
    "for volume in volumes:\n",
    "    with open(folder_edition2 + f\"{volume}.txt\", \"r\", encoding='utf-8') as f:\n",
    "        for line in tqdm(f):\n",
    "            pagenbr_matches = re.search(r'page_number=(\\d+)', line)\n",
    "            if pagenbr_matches:\n",
    "                page_nbr = int(pagenbr_matches.group(1))\n",
    "                # print(f\"page_nbr: {page_nbr}: \")\n",
    "            else:\n",
    "                line = line.rstrip()[:MAX_ENTRY_LENGTH]\n",
    "                if line:\n",
    "                    line = clean_html_markup(line, word_frequencies_remove_tags)\n",
    "                    words = [word.lower() for word in ru.words_in_line(line)]\n",
    "                    total_words += len(words)\n",
    "                    for word in words:\n",
    "                        if word in occurences.keys():\n",
    "                            occurences[word] += 1\n",
    "                        else:\n",
    "                            occurences[word] = 1\n",
    "\n",
    "occurences_file = 'word_frequencies.pickle'\n",
    "\n",
    "with open(occurences_file, 'wb') as handle:\n",
    "    pickle.dump(occurences, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"total_words: {total_words}, occurences[\\\"ett\\\"] = {occurences['ett']}, frequency = {occurences['ett']/ total_words}, freq(monopolsystemet) = {occurences['monopolsystemet'] / total_words} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating (automatically) annotated training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes = edition2_volume_start_end.keys()\n",
    "\n",
    "labeled_data = []\n",
    "\n",
    "frequencies, max_frequency = cu.get_word_frequencies()\n",
    "page_nbr = 0\n",
    "is_entry = False\n",
    "first_letter_list: list[str] = []\n",
    "first_letter_boundary = 0\n",
    "volume_letters_index = -1\n",
    "for volume in volumes:\n",
    "    first_letter_boundary = 0\n",
    "    volume_letters_index = -1\n",
    "    page_nbr = 0\n",
    "    with open(folder_edition2 + f\"{volume}.txt\", \"r\", encoding='utf-8') as f:\n",
    "    \n",
    "        for line in tqdm(f):\n",
    "            pagenbr_matches = re.search(r'page_number=(\\d+)', line)\n",
    "            if pagenbr_matches:\n",
    "                page_nbr = int(pagenbr_matches.group(1))\n",
    "                if page_nbr > first_letter_boundary:\n",
    "                    volume_letters_index += 1\n",
    "                    first_letter_list = edition2_volume_letters[volume][volume_letters_index][0]\n",
    "                    # try:\n",
    "                    first_letter_boundary = edition2_volume_letters[volume][volume_letters_index][1]\n",
    "                    # except:\n",
    "                    #     print(f\"volume = {volume}, volume_letters_index = {volume_letters_index}, page_nbr: {page_nbr}, \")\n",
    "                    #     break\n",
    "            else:\n",
    "                line = line.rstrip()[:MAX_ENTRY_LENGTH]\n",
    "                if line and (len(line) > 40) and (len(line) > 75 or line.find(\". Se \") == -1):\n",
    "                    item = {}\n",
    "                    # --- BOLD MATCHING --- create ground truth\n",
    "                    if line.startswith(tuple([f\"<b>{l}\" for l in first_letter_list])):\n",
    "                        line = clean_html_markup(line, classifier_remove_tags)\n",
    "                        item[\"class\"] = 1\n",
    "                        is_entry = True\n",
    "\n",
    "\n",
    "                    elif line and (not line.startswith(\"Fig. \")) and (not line.startswith(\"Ord, som saknas under K\")) and (not (line[0] in first_letter_list)) and line[0] in ALPHABET:\n",
    "                        item[\"class\"] = 0\n",
    "                        is_entry = True\n",
    "                    \n",
    "                    if is_entry:\n",
    "                        # regexes\n",
    "                        item[\"punctuation_after_first_word\"] = ru.punctuation_after_first_word(line)\n",
    "                        item[\"square_bracket\"] = ru.square_bracket(line)\n",
    "                        item[\"square_bracket_with_punctuation\"] = ru.square_bracket_with_punctuation(line)\n",
    "                        item[\"parentheses\"] = ru.parentheses(line)\n",
    "                        item[\"parentheses_with_punctuation\"] = ru.parentheses_with_punctuation(line)\n",
    "                        item[\"category_word\"] = ru.category_word(line)\n",
    "                        \n",
    "                        #first word frequency\n",
    "                        line = clean_html_markup(line, word_frequencies_remove_tags)\n",
    "                        word = ru.words_in_line(line)[0].lower()\n",
    "                        item[\"first_word_frequency\"] = cu.relative_word_frequency(word, frequencies, max_frequency)                    \n",
    "                        item[\"text\"] = line #this one should be last\n",
    "\n",
    "                        labeled_data.append(item)\n",
    "                        is_entry = False\n",
    "\n",
    "with open('training_data.json', 'w', encoding='utf-8') as outfile:\n",
    "    print(\"MAKE TO JSON\")\n",
    "    json.dump(labeled_data, outfile, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
