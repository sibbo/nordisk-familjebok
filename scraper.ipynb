{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper\n",
    "\n",
    "This notebook is for scraping and segmenting the first and second editions of Nordisk Familjebok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import regex as re\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "INF = 10**9\n",
    "MAX_ENTRY_LENGTH = 200\n",
    "INDEX_SEGMENTER_THRESHOLD = 0.15 #relative edit distance threshold\n",
    "ENCYCLOPEDIAS_FOLDER = \"encyclopedias/\"\n",
    "PAGE_NUMBER_STRING = \"page_number=\"\n",
    "INDEX_STRING = \"index=\"\n",
    "\n",
    "INDEX_BEGIN = \"<b>On this page / på denna sida</b>\\n\"\n",
    "INDEX_END = \"<p>\"\n",
    "\n",
    "DELIM_BEGIN = \"<!-- mode=normal -->\"\n",
    "DELIM_END = \"<!-- NEWIMAGE2 -->\"\n",
    "\n",
    "html_entities = [\n",
    "                [\"&quot;\", \"\\\"\"],\n",
    "                [\"&rsquo;\", \"\\'\"],\n",
    "                [\"&lsquo;\", \"\\'\"],\n",
    "                [\"&ndash;\", \"-\"],\n",
    "                [\"<br>\", \"\"],\n",
    "                ['<span class=\"sp\">', \"\"],\n",
    "                ['<span class=\"overline\">', \"\"],\n",
    "                ['<span class=\"sc\">', \"\"],\n",
    "                [\"</span>\", \"\"],\n",
    "                [\"&lt;\", \"<\"],\n",
    "                [\"&gt;\", \">\"],\n",
    "                [\"&nbsp;\", \" \"],\n",
    "                [\"&amp;\", \"&\"],\n",
    "                ]\n",
    "\n",
    "base_url = \"https://runeberg.org/nf\"\n",
    "# base_url = \"http://runeberg.org/download.pl?mode=ocrtext&work=nf\"\n",
    "\n",
    "#the ranges for the urls, they have a slightly weird format\n",
    "\n",
    "edition1_url_range = {\n",
    "    'a': \"abcdefghijklmnopqr\",\n",
    "}\n",
    "\n",
    "edition2_url_range = {\n",
    "    'b': \"abcdefghijklmnopqrst\",\n",
    "    'c': \"abcdefghijklmn\",\n",
    "}\n",
    "\n",
    "#the start and end pages for each volume\n",
    "edition1_volume_start_end = {\n",
    "    \"aa\": (9, 1579),\n",
    "    \"ab\": (9, 800),\n",
    "    \"ac\": (7, 798),\n",
    "    \"ad\": (7, 797),\n",
    "    \"ae\": (7, 798),\n",
    "    \"af\": (5, 795),\n",
    "    \"ag\": (7, 798),\n",
    "    \"ah\": (5, 799),\n",
    "    \"ai\": (7, 798),\n",
    "    \"aj\": (7, 798),\n",
    "    \"ak\": (7, 798),\n",
    "    \"al\": (7, 798),\n",
    "    \"am\": (7, 798),\n",
    "    \"an\": (7, 798),\n",
    "    \"ao\": (7, 798),\n",
    "    \"ap\": (7, 826),\n",
    "    \"aq\": (5, 804),\n",
    "    \"ar\": (3, 403),\n",
    "}\n",
    "\n",
    "#the pages where the lookup letter changes\n",
    "edition1_volume_letters = {\n",
    "    \"aa\": [(\"A\", 1383), (\"B\", INF)],\n",
    "    \"ab\": [(\"B\", 751), (\"C\", INF)], \n",
    "    \"ac\": [(\"C\", 369), (\"D\", INF)],\n",
    "    \"ad\": [(\"D\", 58), (\"E\", 464), (\"F\", INF)],\n",
    "    \"ae\": [(\"F\", 380), (\"G\", INF)],\n",
    "    \"af\": [(\"G\", 220), (\"H\", INF)],\n",
    "    \"ag\": [(\"H\", 196), (\"I\", 489), (\"J\", 778), (\"K\", INF)],\n",
    "    \"ah\": [(\"K\", INF)], \n",
    "    \"ai\": [(\"K\", 232), (\"L\", INF)], \n",
    "    \"aj\": [(\"L\", 255), (\"M\", INF)],\n",
    "    \"ak\": [(\"M\", 380), (\"N\", INF)],\n",
    "    \"al\": [(\"N\", 30), (\"O\", 277), (\"P\", INF)],\n",
    "    \"am\": [(\"P\", 262), (\"Q\", 306), (\"R\", INF)],\n",
    "    \"an\": [(\"R\", 147), (\"S\", INF)],\n",
    "    \"ao\": [(\"S\", 641), (\"T\", INF)],\n",
    "    \"ap\": [(\"T\", 625), (\"U\", INF)], #special case for Ü\n",
    "    \"aq\": [(\"V\", INF)], #special case with W\n",
    "    \"ar\": [(\"V\", 35), (\"X\", 42), (\"Y\", 78), (\"Z\", 178), (\"Å\", 243), (\"Ä\", 277), (\"Ö\", INF)] #special case with W\n",
    "}\n",
    "\n",
    "#the start and end pages for each volume\n",
    "edition2_volume_start_end = {\n",
    "    \"ba\": (13, 824),\n",
    "    \"bb\": (13, 798),\n",
    "    \"bc\": (17, 808),\n",
    "    \"bd\": (17, 814),\n",
    "    \"be\": (17, 800),\n",
    "    \"bf\": (17, 814),\n",
    "    \"bg\": (17, 802),\n",
    "    \"bh\": (17, 806),\n",
    "    \"bi\": (17, 782),\n",
    "    \"bj\": (17, 804),\n",
    "    \"bk\": (17, 784),\n",
    "    \"bl\": (17, 816),\n",
    "    \"bm\": (17, 784),\n",
    "    \"bn\": (17, 784),\n",
    "    \"bo\": (17, 788),\n",
    "    \"bp\": (17, 812),\n",
    "    \"bq\": (17, 785),\n",
    "    \"br\": (17, 779),\n",
    "    \"bs\": (17, 820),\n",
    "    \"bt\": (17, 796),\n",
    "    \"ca\": (17, 812),\n",
    "    \"cb\": (17, 778),\n",
    "    \"cc\": (17, 817),\n",
    "    \"cd\": (17, 784),\n",
    "    \"ce\": (17, 794),\n",
    "    \"cf\": (17, 820),\n",
    "    \"cg\": (17, 806),\n",
    "    \"ch\": (17, 688),\n",
    "    \"ci\": (17, 458),\n",
    "    \"cj\": (17, 719),\n",
    "    \"ck\": (17, 688),\n",
    "    \"cl\": (17, 686),\n",
    "    \"cm\": (17, 685),\n",
    "    \"cn\": (17, 180),\n",
    "}\n",
    "\n",
    "#the pages where the lookup letter changes\n",
    "edition2_volume_letters = {\n",
    "    \"ba\": [(\"A\", INF)],\n",
    "    \"bb\": [(\"A\", 310), (\"B\", INF)],\n",
    "    \"bc\": [(\"B\", INF)],\n",
    "    \"bd\": [(\"B\", 519), (\"C\", INF)],\n",
    "    \"be\": [(\"C\", 558), (\"D\", INF)],\n",
    "    \"bf\": [(\"D\", 678), (\"E\", INF)],\n",
    "    \"bg\": [(\"E\", 651), (\"F\", INF)],\n",
    "    \"bh\": [(\"F\", INF)],\n",
    "    \"bi\": [(\"F\", 281), (\"G\", INF)],\n",
    "    \"bj\": [(\"G\", 506), (\"H\", INF)],\n",
    "    \"bk\": [(\"H\", INF)],\n",
    "    \"bl\": [(\"H\", 180), (\"I\", 611), (\"J\", INF)],\n",
    "    \"bm\": [(\"J\", 275), (\"K\", INF)],\n",
    "    \"bn\": [(\"K\", INF)],\n",
    "    \"bo\": [(\"K\", 385), (\"L\", INF)],\n",
    "    \"bp\": [(\"L\",  INF)],\n",
    "    \"bq\": [(\"L\", 180), (\"M\", INF)],\n",
    "    \"br\": [(\"M\", INF)],\n",
    "    \"bs\": [(\"M\", 213), (\"N\", INF)],\n",
    "    \"bt\": [(\"N\", 213), (\"O\", 641), (\"P\", INF)],\n",
    "    \"ca\": [(\"P\", INF)],\n",
    "    \"cb\": [(\"P\", 385), (\"Q\", 418), (\"R\", INF)],\n",
    "    \"cc\": [(\"R\", INF)],\n",
    "    \"cd\": [(\"R\", 136), (\"S\"), INF],\n",
    "    \"ce\": [(\"S\", INF)],\n",
    "    \"cf\": [(\"S\", INF)],\n",
    "    \"cg\": [(\"S\", INF)],\n",
    "    \"ch\": [(\"S\", 138), (\"T\", INF)],\n",
    "    \"ci\": [(\"T\", INF)],\n",
    "    \"cj\": [(\"T\", 441), (\"U\", INF)],\n",
    "    \"ck\": [(\"U\", 116), (\"V\", INF)],\n",
    "    \"cl\": [(\"V\", INF)], #Special case with W\n",
    "    \"cm\": [(\"V\", 281), (\"X\", 291), (\"Y\", 357), (\"Z\", 488), (\"Å\", 619), (\"Ä\", INF)], #special case with W?\n",
    "    \"cn\": [(\"Ö\", INF)]\n",
    "}\n",
    "\n",
    "#folder to save the .txt files in\n",
    "folder_edition1 = ENCYCLOPEDIAS_FOLDER + \"first/\"\n",
    "folder_edition2 = ENCYCLOPEDIAS_FOLDER + \"second/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_substring_between_delimiters(s: str, start: str, end: str) -> str:\n",
    "    start_index = s.find(start)\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "    start_index += len(start)\n",
    "    end_index = s.find(end, start_index)\n",
    "    if end_index == -1:\n",
    "        return None\n",
    "\n",
    "    return s[start_index: end_index]\n",
    "\n",
    "def clean_html_markup(s: str, html_entities: list) -> str:\n",
    "    res = s\n",
    "    for pair in html_entities:\n",
    "        res = res.replace(pair[0], pair[1])\n",
    "    return res\n",
    "\n",
    "def remove_single_newline(s: str) -> str:\n",
    "    return re.sub(r'(?<!\\n)\\n(?!\\n)|(\\n+)(?=\\n)', ' ', s)\n",
    "\n",
    "def scrape_page_text_and_index(url: str) -> tuple[str, str]:\n",
    "    try:\n",
    "        page = urlopen(url)\n",
    "    except:\n",
    "        return None, None\n",
    "    html = page.read().decode(\"utf-8\")\n",
    "    index = get_substring_between_delimiters(html, INDEX_BEGIN, INDEX_END)\n",
    "    html = get_substring_between_delimiters(html, DELIM_BEGIN, DELIM_END)\n",
    "    if not index == None:\n",
    "        index = clean_html_markup(index, html_entities)\n",
    "        index = remove_single_newline(index)\n",
    "    if not html == None:\n",
    "        html = clean_html_markup(html, html_entities)\n",
    "        html = remove_single_newline(html)\n",
    "    return html, index\n",
    "\n",
    "def create_url(partial_url: str, i: int) -> str:\n",
    "    return partial_url + f\"{i:04d}\" + \".html\"\n",
    "\n",
    "def scrape_volume(base_url: str, volume_start_number: int, volume_end_number: int = 9999999) -> str:\n",
    "    i = volume_start_number\n",
    "    volume_str: str = \"\"\n",
    "    while(i <= volume_end_number):\n",
    "        url = create_url(base_url, i)\n",
    "        text, index = scrape_page_text_and_index(url)\n",
    "        if text == None or index == None:\n",
    "            i += 1\n",
    "            continue\n",
    "        volume_str += PAGE_NUMBER_STRING + str(i) + \", \"\n",
    "        volume_str += INDEX_STRING + index + \"\\n\"\n",
    "        volume_str += text\n",
    "        print(f\"i = {i}: {volume_str[-10:]}\")\n",
    "        i += 1\n",
    "    return volume_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the first edition (1800-tals utgåvan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SCRAPING ---\n",
    "first_letter = 'a'\n",
    "for second_letter in edition1_url_range[first_letter]:\n",
    "    volume_index = first_letter + second_letter\n",
    "    f = open(folder_edition1 + volume_index + \".txt\", \"w\")\n",
    "    volume_url = base_url + volume_index + \"/\"\n",
    "    print(volume_url)\n",
    "    f.write(scrape_volume(volume_url, \n",
    "        volume_start_number=edition1_volume_start_end[volume_index][0], \n",
    "        volume_end_number=edition1_volume_start_end[volume_index][1])) \n",
    "    print(f\"volume index: {volume_index}\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the second edition (ugglan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SCRAPING ---\n",
    "for first_letter in ('b', 'c'):\n",
    "    for second_letter in edition2_url_range[first_letter]:\n",
    "        volume_index = first_letter + second_letter\n",
    "        f = open(folder_edition2 + volume_index + \".txt\", \"w\")\n",
    "        volume_url = base_url + volume_index + \"/\"\n",
    "        f.write(scrape_volume(volume_url, \n",
    "            volume_start_number=edition2_volume_start_end[volume_index][0], \n",
    "            volume_end_number=edition2_volume_start_end[volume_index][1])) \n",
    "        print(f\"volume index: {volume_index}\")\n",
    "        print(volume_url)\n",
    "        f.close()\n",
    "\n",
    "# volume_index = 'bo'\n",
    "# f = open(folder + volume_index + \".txt\", \"w\", encoding='utf-8')\n",
    "# print(f\"volume index: {volume_index}\")\n",
    "# volume_url = base_url + volume_index + \"/\"\n",
    "# print(volume_url)\n",
    "# if volume_index in [\"ba\", \"bb\"]:\n",
    "#     text = scrape_volume(volume_url, volume_start_number_ba_bb)\n",
    "#     print(text)\n",
    "#     f.write(text)\n",
    "# else:\n",
    "#     f.write(scrape_volume(volume_url, volume_start_number))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for entry classification using the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_string_to_list(index: str) -> list[str]:\n",
    "    return [query.strip() for query in index.split(\" - \")][1:]\n",
    "\n",
    "def clean_text_and_index(text_word: str, index_word: str) -> str:\n",
    "    # Clean text_word, e.g., remove italic tags, [...].\n",
    "    tags = [\n",
    "                [\"<b>\", \"\"],\n",
    "                [\"</b>\", \"\"],\n",
    "                [\"<i>\", \"\"],\n",
    "                [\"</i>\", \"\"],\n",
    "                ]\n",
    "    text_word = clean_html_markup(text_word, tags)\n",
    "\n",
    "    #if not '[' in index_word:\n",
    "    text_word = re.sub(r'\\s*\\[(.*?)\\]', '', text_word)\n",
    "    index_word = re.sub(r'\\s*\\[(.*?)\\]', '', index_word)    \n",
    "    #if not '(' in index_word:\n",
    "    text_word = re.sub(r'\\s*\\((.*?)\\)', '', text_word)\n",
    "    index_word = re.sub(r'\\s*\\((.*?)\\)', '', index_word)\n",
    "\n",
    "    return text_word, index_word\n",
    "\n",
    "def edit_distance(text_word: str, index_word: str) -> int:\n",
    "    \n",
    "    #Initializing distance matrix\n",
    "    distances = np.zeros((len(text_word) + 1, len(index_word) + 1))\n",
    "    for t1 in range(len(text_word) + 1):\n",
    "        distances[t1][0] = t1\n",
    "    for t2 in range(len(index_word) + 1):\n",
    "        distances[0][t2] = t2\n",
    "\n",
    "    # Computation\n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    \n",
    "    for t1 in range(1, len(text_word) + 1):\n",
    "        for t2 in range(1, len(index_word) + 1):\n",
    "            if (text_word[t1-1] == index_word[t2-1]):\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "            else:\n",
    "                a = distances[t1][t2 - 1]\n",
    "                b = distances[t1 - 1][t2]\n",
    "                c = distances[t1 - 1][t2 - 1]\n",
    "                \n",
    "                if (a <= b and a <= c):\n",
    "                    distances[t1][t2] = a + 1\n",
    "                elif (b <= a and b <= c):\n",
    "                    distances[t1][t2] = b + 1\n",
    "                else:\n",
    "                    distances[t1][t2] = c + 1\n",
    "\n",
    "    return distances[len(text_word)][len(index_word)]\n",
    "\n",
    "def print_distances(distances, token1_length, token2_length):\n",
    "    for t1 in range(token1_length + 1):\n",
    "        for t2 in range(token2_length + 1):\n",
    "            print(int(distances[t1][t2]), end=\" \")\n",
    "        print()\n",
    "\n",
    "def relative_edit_distance(text_word: str, index_word: str) -> float:\n",
    "    return edit_distance(text_word, index_word) / len(index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_index = \"bo\"\n",
    "\n",
    "volume = open(folder_edition2 + f\"{volume_index}.txt\", \"r\", encoding='utf-8')\n",
    "json_file = open(\"nf.json\", 'a', encoding='utf-8')\n",
    "\n",
    "#loopa igenom hela filen, om raden har bold tags så tar vi tills\n",
    "#nästa newline eller de första 200 tecknen, den som kommer först\n",
    "#och sparar till en json-fil\n",
    "\n",
    "#TEMPORARY\n",
    "edition_nbr = 2\n",
    "volume_nbr = 1\n",
    "\n",
    "data = []\n",
    "entry_nbr = 0\n",
    "page_nbr = 0\n",
    "index = []\n",
    "is_entry = False\n",
    "bold_hits = 0\n",
    "index_hits = 0\n",
    "classifier_hits = 0\n",
    "first_letter = \"\"\n",
    "first_letter_boundary = 0\n",
    "volume_letters_index = -1\n",
    "for line in volume:\n",
    "    entryid = f\"e{edition_nbr}_v{volume_nbr}_{page_nbr}_{entry_nbr}\"\n",
    "    pagenbr_matches = re.search(r'page_number=(\\d+)', line)\n",
    "    if pagenbr_matches:\n",
    "        page_nbr = int(pagenbr_matches.group(1))\n",
    "        if page_nbr > first_letter_boundary:\n",
    "            volume_letters_index += 1\n",
    "            first_letter = edition2_volume_letters[volume_index][volume_letters_index][0]\n",
    "            first_letter_boundary = edition2_volume_letters[volume_index][volume_letters_index][1]\n",
    "        entry_nbr = 0\n",
    "        start_index = line.find(INDEX_STRING)\n",
    "        index = index_string_to_list(line[start_index + len(INDEX_STRING):]) #Identical for lines on same page\n",
    "        index = sorted(index, key=len, reverse=True) # To solve problem (Arm, Armadillo)\n",
    "        print(f\"page_nbr: {page_nbr}: \", index)\n",
    "    else:\n",
    "        line = line.rstrip()[:MAX_ENTRY_LENGTH] # :200\n",
    "        \n",
    "        # --- BOLD MATCHING ---\n",
    "        if line.startswith(\"<b>\"):\n",
    "            is_entry = True\n",
    "            headword = \"\"\n",
    "            matches = re.findall(r'<b>(.*?)<\\/b>', line)\n",
    "            bold_hits += 1\n",
    "            if matches:\n",
    "                headword = re.sub(r'[,.]$', '', matches[0])\n",
    "\n",
    "        elif line.startswith(first_letter) and (len(line) > 40 or \" Se \" in line): # Removing special case\n",
    "\n",
    "            # --- INDEX MATCHING ---    \n",
    "            if index and not \"...\" in index[0]: # index \n",
    "                smallest_dist = INF\n",
    "                smallest_index = -1\n",
    "                for i, index_word in enumerate(index):\n",
    "                    temp_line, temp_index = clean_text_and_index(line, index_word)\n",
    "                    if relative_edit_distance(temp_line[:len(temp_index)], temp_index) < INDEX_SEGMENTER_THRESHOLD: \n",
    "                        headword = index_word\n",
    "                        is_entry = True\n",
    "                        index.pop(i)\n",
    "                        index_hits += 1\n",
    "                        print(f\"Line = {line[:20]}, Index_word: {headword}\")\n",
    "                        break\n",
    "            \n",
    "            # --- CLASSIFIER MATCHING ---\n",
    "            # elif line[0] not capital and not current uppslagsbokstav\n",
    "            #else: use index or neural network\n",
    "            \n",
    "            \n",
    "        if is_entry:\n",
    "            item = {\n",
    "                \"headword\": headword,\n",
    "                \"entryid\": entryid,\n",
    "                \"text\": line,\n",
    "                \"type\": 0,\n",
    "                \"qid\": \"0\",\n",
    "                \"first_edition_key\": \"\",\n",
    "                \"fourth_edition_key\": \"\"\n",
    "            }\n",
    "            data.append(item)\n",
    "            entry_nbr += 1\n",
    "            is_entry = False\n",
    "        else: \n",
    "            print(f\"NOT FOUND FOR: line = {line[:20]}\")\n",
    "\n",
    "        \n",
    "\n",
    "json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "print(f\"Bold hits: {bold_hits}\")\n",
    "print(f\"Index hits: {index_hits}\")\n",
    "print(f\"Classifier hits: {classifier_hits}\")\n",
    "\n",
    "volume.close()\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial test for creating annotated training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take some real data, create regexes that can extract some features\n",
    "\n",
    "#Comma or period immediately after first word\n",
    "# r\"^[\\p{L}\\'\\-]+(?=\\,|\\.)\"\n",
    "\n",
    "#[ character within first 40 characters\n",
    "# r\"^.{0,40}\\[\"\n",
    "\n",
    "#[ character within first 40 characters, comma or period after ]\n",
    "# r\"^.{0,40}\\[.{1,20}?\\](?=\\,|\\.)\"\n",
    "\n",
    "#( character within first 40 characters\n",
    "# r\"^.{0,40}\\(\"\n",
    "\n",
    "#( character within first 40 characters, comma or period after )\n",
    "# r\"^.{0,40}\\(.{1,20}?\\)(?=\\,|\\.)\"\n",
    "\n",
    "#Category word (mus. , bygnk. , kem. ) after first comma or period\n",
    "# r\"^.{1,70}, \\p{L}{1,11}\\.\"\n",
    "\n",
    "\n",
    "\n",
    "classifier_remove_tags = [\n",
    "    [\"<b>\", \"\"],\n",
    "    [\"</b>\", \"\"],\n",
    "]\n",
    "\n",
    "volume = open(folder_edition2 + \"bo.txt\", \"r\", encoding='utf-8')\n",
    "\n",
    "labeled_data = []\n",
    "\n",
    "i = 1\n",
    "page_nbr = 143\n",
    "for line in volume:\n",
    "    if i > 1900:\n",
    "        break\n",
    "    if i > 1750:\n",
    "        pagenbr_matches = re.search(r'page_number=(\\d+)', line)\n",
    "        if pagenbr_matches:\n",
    "            page_nbr = int(pagenbr_matches.group(1))\n",
    "            print(f\"page_nbr: {page_nbr}: \")\n",
    "        else:\n",
    "            line = line.rstrip()[:MAX_ENTRY_LENGTH] # maybe remove?\n",
    "            item = {}\n",
    "            # regexes\n",
    "\n",
    "\n",
    "            \n",
    "            # --- BOLD MATCHING --- create ground truth\n",
    "            if line.startswith(\"<b>\"):\n",
    "                line = clean_html_markup(line, classifier_remove_tags)     \n",
    "                item[\"class\"] = 1\n",
    "            else:\n",
    "                item[\"class\"] = 0\n",
    "            item[\"text\"] = line\n",
    "            labeled_data.append(item)\n",
    "        \n",
    "    i += 1\n",
    "\n",
    "with open('training_data.txt', 'w', encoding='utf-8') as outfile:\n",
    "    print(\"MAKE TO JSON\")\n",
    "    for line in labeled_data:\n",
    "        json.dump(labeled_data, outfile, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
