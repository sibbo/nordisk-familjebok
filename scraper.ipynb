{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper\n",
    "\n",
    "This notebook is for scraping and segmenting the first and second editions of Nordisk Familjebok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import regex as re\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "INF = 10**9\n",
    "MAX_ENTRY_LENGTH = 200\n",
    "INDEX_SEGMENTER_THRESHOLD = 0.15 #relative edit distance threshold\n",
    "ENCYCLOPEDIAS_FOLDER = \"encyclopedias/\"\n",
    "PAGE_NUMBER_STRING = \"page_number=\"\n",
    "INDEX_STRING = \"index=\"\n",
    "\n",
    "INDEX_BEGIN = \"<b>On this page / på denna sida</b>\\n\"\n",
    "INDEX_END = \"<p>\"\n",
    "\n",
    "DELIM_BEGIN = \"<!-- mode=normal -->\"\n",
    "DELIM_END = \"<!-- NEWIMAGE2 -->\"\n",
    "\n",
    "html_entities = [\n",
    "                [\"&quot;\", \"\\\"\"],\n",
    "                [\"&rsquo;\", \"\\'\"],\n",
    "                [\"&lsquo;\", \"\\'\"],\n",
    "                [\"&ndash;\", \"-\"],\n",
    "                [\"<br>\", \"\"],\n",
    "                ['<span class=\"sp\">', \"\"],\n",
    "                ['<span class=\"overline\">', \"\"],\n",
    "                ['<span class=\"sc\">', \"\"],\n",
    "                [\"</span>\", \"\"],\n",
    "                [\"&lt;\", \"<\"],\n",
    "                [\"&gt;\", \">\"],\n",
    "                [\"&nbsp;\", \" \"],\n",
    "                [\"&amp;\", \"&\"],\n",
    "                ]\n",
    "\n",
    "base_url = \"https://runeberg.org/nf\"\n",
    "# base_url = \"http://runeberg.org/download.pl?mode=ocrtext&work=nf\"\n",
    "\n",
    "#the ranges for the urls, they have a slightly weird format\n",
    "\n",
    "edition1_url_range = {\n",
    "    'a': \"abcdefghijklmnopqr\",\n",
    "}\n",
    "\n",
    "edition2_url_range = {\n",
    "    'b': \"abcdefghijklmnopqrst\",\n",
    "    'c': \"abcdefghijklmn\",\n",
    "}\n",
    "\n",
    "#the start and end pages for each volume\n",
    "edition1_volume_start_end = {\n",
    "    \"aa\": (9, 1579),\n",
    "    \"ab\": (9, 800),\n",
    "    \"ac\": (7, 798),\n",
    "    \"ad\": (7, 797),\n",
    "    \"ae\": (7, 798),\n",
    "    \"af\": (5, 795),\n",
    "    \"ag\": (7, 798),\n",
    "    \"ah\": (5, 799),\n",
    "    \"ai\": (7, 798),\n",
    "    \"aj\": (7, 798),\n",
    "    \"ak\": (7, 798),\n",
    "    \"al\": (7, 798),\n",
    "    \"am\": (7, 798),\n",
    "    \"an\": (7, 798),\n",
    "    \"ao\": (7, 798),\n",
    "    \"ap\": (7, 826),\n",
    "    \"aq\": (5, 804),\n",
    "    \"ar\": (3, 403),\n",
    "}\n",
    "\n",
    "#the pages where the lookup letter changes\n",
    "edition1_volume_letters = {\n",
    "    \"aa\": [([\"A\"], 1383), ([\"B\"], INF)],\n",
    "    \"ab\": [([\"B\"], 751), ([\"C\"], INF)], \n",
    "    \"ac\": [([\"C\"], 369), ([\"D\"], INF)],\n",
    "    \"ad\": [([\"D\"], 58), ([\"E\"], 464), ([\"F\"], INF)],\n",
    "    \"ae\": [([\"F\"], 380), ([\"G\"], INF)],\n",
    "    \"af\": [([\"G\"], 220), ([\"H\"], INF)],\n",
    "    \"ag\": [([\"H\"], 196), ([\"I\"], 489), ([\"J\"], 778), ([\"K\"], INF)],\n",
    "    \"ah\": [([\"K\"], INF)], \n",
    "    \"ai\": [([\"K\"], 232), ([\"L\"], INF)], \n",
    "    \"aj\": [([\"L\"], 255), ([\"M\"], INF)],\n",
    "    \"ak\": [([\"M\"], 380), ([\"N\"], INF)],\n",
    "    \"al\": [([\"N\"], 30), ([\"O\"], 277), ([\"P\"], INF)],\n",
    "    \"am\": [([\"P\"], 262), ([\"Q\"], 306), ([\"R\"], INF)],\n",
    "    \"an\": [([\"R\"], 147), ([\"S\"], INF)],\n",
    "    \"ao\": [([\"S\"], 641), ([\"T\"], INF)],\n",
    "    \"ap\": [([\"T\"], 625), ([\"U\", \"Ü\"], INF)], #special case for Ü\n",
    "    \"aq\": [([\"V\", \"W\"], INF)], #special case with W\n",
    "    \"ar\": [([\"V\", \"W\"], 35), ([\"X\"], 42), ([\"Y\"], 78), ([\"Z\"], 178), ([\"Å\"], 243), ([\"Ä\"], 277), ([\"Ö\"], INF)] #special case with W\n",
    "}\n",
    "\n",
    "#the start and end pages for each volume\n",
    "edition2_volume_start_end = {\n",
    "    \"ba\": (13, 824),\n",
    "    \"bb\": (13, 798),\n",
    "    \"bc\": (17, 808),\n",
    "    \"bd\": (17, 814),\n",
    "    \"be\": (17, 800),\n",
    "    \"bf\": (17, 814),\n",
    "    \"bg\": (17, 802),\n",
    "    \"bh\": (17, 806),\n",
    "    \"bi\": (17, 782),\n",
    "    \"bj\": (17, 804),\n",
    "    \"bk\": (17, 784),\n",
    "    \"bl\": (17, 816),\n",
    "    \"bm\": (17, 784),\n",
    "    \"bn\": (17, 784),\n",
    "    \"bo\": (17, 788),\n",
    "    \"bp\": (17, 812),\n",
    "    \"bq\": (17, 785),\n",
    "    \"br\": (17, 779),\n",
    "    \"bs\": (17, 820),\n",
    "    \"bt\": (17, 796),\n",
    "    \"ca\": (17, 812),\n",
    "    \"cb\": (17, 778),\n",
    "    \"cc\": (17, 817),\n",
    "    \"cd\": (17, 784),\n",
    "    \"ce\": (17, 794),\n",
    "    \"cf\": (17, 820),\n",
    "    \"cg\": (17, 806),\n",
    "    \"ch\": (17, 688),\n",
    "    \"ci\": (17, 458),\n",
    "    \"cj\": (17, 719),\n",
    "    \"ck\": (17, 688),\n",
    "    \"cl\": (17, 686),\n",
    "    \"cm\": (17, 685),\n",
    "    \"cn\": (17, 180),\n",
    "}\n",
    "\n",
    "#the pages where the lookup letter changes\n",
    "edition2_volume_letters = {\n",
    "    \"ba\": [([\"A\"], INF)],\n",
    "    \"bb\": [([\"A\"], 310), ([\"B\"], INF)],\n",
    "    \"bc\": [([\"B\"], INF)],\n",
    "    \"bd\": [([\"B\"], 519), ([\"C\"], INF)],\n",
    "    \"be\": [([\"C\"], 558), ([\"D\"], INF)],\n",
    "    \"bf\": [([\"D\"], 678), ([\"E\"], INF)],\n",
    "    \"bg\": [([\"E\"], 651), ([\"F\"], INF)],\n",
    "    \"bh\": [([\"F\"], INF)],\n",
    "    \"bi\": [([\"F\"], 281), ([\"G\"], INF)],\n",
    "    \"bj\": [([\"G\"], 506), ([\"H\"], INF)],\n",
    "    \"bk\": [([\"H\"], INF)],\n",
    "    \"bl\": [([\"H\"], 180), ([\"I\"], 611), ([\"J\"], INF)],\n",
    "    \"bm\": [([\"J\"], 275), ([\"K\"], INF)],\n",
    "    \"bn\": [([\"K\"], INF)],\n",
    "    \"bo\": [([\"K\"], 385), ([\"L\"], INF)],\n",
    "    \"bp\": [([\"L\"],  INF)],\n",
    "    \"bq\": [([\"L\"], 180), ([\"M\"], INF)],\n",
    "    \"br\": [([\"M\"], INF)],\n",
    "    \"bs\": [([\"M\"], 213), ([\"N\"], INF)],\n",
    "    \"bt\": [([\"N\"], 213), ([\"O\"], 641), ([\"P\"], INF)],\n",
    "    \"ca\": [([\"P\"], INF)],\n",
    "    \"cb\": [([\"P\"], 385), ([\"Q\"], 418), ([\"R\"], INF)],\n",
    "    \"cc\": [([\"R\"], INF)],\n",
    "    \"cd\": [([\"R\"], 136), ([\"S\"]), INF],\n",
    "    \"ce\": [([\"S\"], INF)],\n",
    "    \"cf\": [([\"S\"], INF)],\n",
    "    \"cg\": [([\"S\"], INF)],\n",
    "    \"ch\": [([\"S\"], 138), ([\"T\"], INF)],\n",
    "    \"ci\": [([\"T\"], INF)],\n",
    "    \"cj\": [([\"T\"], 441), ([\"U\"], INF)],\n",
    "    \"ck\": [([\"U\"], 116), ([\"V\"], INF)],\n",
    "    \"cl\": [([\"V\", \"W\"], INF)], #Special case with W\n",
    "    \"cm\": [([\"V\", \"W\"], 281), ([\"X\"], 291), ([\"Y\"], 357), ([\"Z\"], 488), ([\"Å\"], 619), ([\"Ä\"], INF)], #special case with W?\n",
    "    \"cn\": [([\"Ö\"], INF)]\n",
    "}\n",
    "\n",
    "#folder to save the .txt files in\n",
    "folder_edition1 = ENCYCLOPEDIAS_FOLDER + \"first/\"\n",
    "folder_edition2 = ENCYCLOPEDIAS_FOLDER + \"second/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_substring_between_delimiters(s: str, start: str, end: str) -> str:\n",
    "    start_index = s.find(start)\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "    start_index += len(start)\n",
    "    end_index = s.find(end, start_index)\n",
    "    if end_index == -1:\n",
    "        return None\n",
    "\n",
    "    return s[start_index: end_index]\n",
    "\n",
    "def clean_html_markup(s: str, html_entities: list) -> str:\n",
    "    res = s\n",
    "    for pair in html_entities:\n",
    "        res = res.replace(pair[0], pair[1])\n",
    "    return res\n",
    "\n",
    "def remove_single_newline(s: str) -> str:\n",
    "    return re.sub(r'(?<!\\n)\\n(?!\\n)|(\\n+)(?=\\n)', ' ', s)\n",
    "\n",
    "def scrape_page_text_and_index(url: str) -> tuple[str, str]:\n",
    "    try:\n",
    "        page = urlopen(url)\n",
    "    except:\n",
    "        return None, None\n",
    "    html = page.read().decode(\"utf-8\")\n",
    "    index = get_substring_between_delimiters(html, INDEX_BEGIN, INDEX_END)\n",
    "    html = get_substring_between_delimiters(html, DELIM_BEGIN, DELIM_END)\n",
    "    if not index == None:\n",
    "        index = clean_html_markup(index, html_entities)\n",
    "        index = remove_single_newline(index)\n",
    "    if not html == None:\n",
    "        html = clean_html_markup(html, html_entities)\n",
    "        html = remove_single_newline(html)\n",
    "    return html, index\n",
    "\n",
    "def create_url(partial_url: str, i: int) -> str:\n",
    "    return partial_url + f\"{i:04d}\" + \".html\"\n",
    "\n",
    "def scrape_volume(base_url: str, volume_start_number: int, volume_end_number: int = 9999999) -> str:\n",
    "    i = volume_start_number\n",
    "    volume_str: str = \"\"\n",
    "    while(i <= volume_end_number):\n",
    "        url = create_url(base_url, i)\n",
    "        text, index = scrape_page_text_and_index(url)\n",
    "        if text == None or index == None:\n",
    "            i += 1\n",
    "            continue\n",
    "        volume_str += PAGE_NUMBER_STRING + str(i) + \", \"\n",
    "        volume_str += INDEX_STRING + index + \"\\n\"\n",
    "        volume_str += text\n",
    "        print(f\"i = {i}: {volume_str[-10:]}\")\n",
    "        i += 1\n",
    "    return volume_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the first edition (1800-tals utgåvan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SCRAPING ---\n",
    "first_letter_list = 'a'\n",
    "for second_letter in edition1_url_range[first_letter_list]:\n",
    "    volume_index = first_letter_list + second_letter\n",
    "    f = open(folder_edition1 + volume_index + \".txt\", \"w\")\n",
    "    volume_url = base_url + volume_index + \"/\"\n",
    "    print(volume_url)\n",
    "    f.write(scrape_volume(volume_url, \n",
    "        volume_start_number=edition1_volume_start_end[volume_index][0], \n",
    "        volume_end_number=edition1_volume_start_end[volume_index][1])) \n",
    "    print(f\"volume index: {volume_index}\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the second edition (ugglan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SCRAPING ---\n",
    "for first_letter_list in ('b', 'c'):\n",
    "    for second_letter in edition2_url_range[first_letter_list]:\n",
    "        volume_index = first_letter_list + second_letter\n",
    "        f = open(folder_edition2 + volume_index + \".txt\", \"w\")\n",
    "        volume_url = base_url + volume_index + \"/\"\n",
    "        f.write(scrape_volume(volume_url, \n",
    "            volume_start_number=edition2_volume_start_end[volume_index][0], \n",
    "            volume_end_number=edition2_volume_start_end[volume_index][1])) \n",
    "        print(f\"volume index: {volume_index}\")\n",
    "        print(volume_url)\n",
    "        f.close()\n",
    "\n",
    "# volume_index = 'bo'\n",
    "# f = open(folder + volume_index + \".txt\", \"w\", encoding='utf-8')\n",
    "# print(f\"volume index: {volume_index}\")\n",
    "# volume_url = base_url + volume_index + \"/\"\n",
    "# print(volume_url)\n",
    "# if volume_index in [\"ba\", \"bb\"]:\n",
    "#     text = scrape_volume(volume_url, volume_start_number_ba_bb)\n",
    "#     print(text)\n",
    "#     f.write(text)\n",
    "# else:\n",
    "#     f.write(scrape_volume(volume_url, volume_start_number))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for entry classification using the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_string_to_list(index: str) -> list[str]:\n",
    "    return [query.strip() for query in index.split(\" - \")][1:]\n",
    "\n",
    "def clean_text_and_index(text_word: str, index_word: str) -> str:\n",
    "    # Clean text_word, e.g., remove italic tags, [...].\n",
    "    tags = [\n",
    "                [\"<b>\", \"\"],\n",
    "                [\"</b>\", \"\"],\n",
    "                [\"<i>\", \"\"],\n",
    "                [\"</i>\", \"\"],\n",
    "                ]\n",
    "    text_word = clean_html_markup(text_word, tags)\n",
    "\n",
    "    #if not '[' in index_word:\n",
    "    text_word = re.sub(r'\\s*\\[(.*?)\\]', '', text_word)\n",
    "    index_word = re.sub(r'\\s*\\[(.*?)\\]', '', index_word)    \n",
    "    #if not '(' in index_word:\n",
    "    text_word = re.sub(r'\\s*\\((.*?)\\)', '', text_word)\n",
    "    index_word = re.sub(r'\\s*\\((.*?)\\)', '', index_word)\n",
    "\n",
    "    return text_word, index_word\n",
    "\n",
    "def edit_distance(text_word: str, index_word: str) -> int:\n",
    "    \n",
    "    #Initializing distance matrix\n",
    "    distances = np.zeros((len(text_word) + 1, len(index_word) + 1))\n",
    "    for t1 in range(len(text_word) + 1):\n",
    "        distances[t1][0] = t1\n",
    "    for t2 in range(len(index_word) + 1):\n",
    "        distances[0][t2] = t2\n",
    "\n",
    "    # Computation\n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    \n",
    "    for t1 in range(1, len(text_word) + 1):\n",
    "        for t2 in range(1, len(index_word) + 1):\n",
    "            if (text_word[t1-1] == index_word[t2-1]):\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "            else:\n",
    "                a = distances[t1][t2 - 1]\n",
    "                b = distances[t1 - 1][t2]\n",
    "                c = distances[t1 - 1][t2 - 1]\n",
    "                \n",
    "                if (a <= b and a <= c):\n",
    "                    distances[t1][t2] = a + 1\n",
    "                elif (b <= a and b <= c):\n",
    "                    distances[t1][t2] = b + 1\n",
    "                else:\n",
    "                    distances[t1][t2] = c + 1\n",
    "\n",
    "    return distances[len(text_word)][len(index_word)]\n",
    "\n",
    "def print_distances(distances, token1_length, token2_length):\n",
    "    for t1 in range(token1_length + 1):\n",
    "        for t2 in range(token2_length + 1):\n",
    "            print(int(distances[t1][t2]), end=\" \")\n",
    "        print()\n",
    "\n",
    "def relative_edit_distance(text_word: str, index_word: str) -> float:\n",
    "    return edit_distance(text_word, index_word) / len(index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_index = \"bo\"\n",
    "\n",
    "volume = open(folder_edition2 + f\"{volume_index}.txt\", \"r\", encoding='utf-8')\n",
    "json_file = open(\"nf.json\", 'a', encoding='utf-8')\n",
    "\n",
    "#TEMPORARY\n",
    "edition_nbr = 2\n",
    "volume_nbr = 1\n",
    "\n",
    "data = []\n",
    "entry_nbr = 0\n",
    "page_nbr = 0\n",
    "index = []\n",
    "is_entry = False\n",
    "bold_hits = 0\n",
    "index_hits = 0\n",
    "classifier_hits = 0\n",
    "first_letter_list: list[str] = []\n",
    "first_letter_boundary = 0\n",
    "volume_letters_index = -1\n",
    "for line in tqdm(volume):\n",
    "    entryid = f\"e{edition_nbr}_v{volume_nbr}_{page_nbr}_{entry_nbr}\"\n",
    "    pagenbr_matches = re.search(r'page_number=(\\d+)', line)\n",
    "    if pagenbr_matches:\n",
    "        page_nbr = int(pagenbr_matches.group(1))\n",
    "        if page_nbr > first_letter_boundary:\n",
    "            volume_letters_index += 1\n",
    "            first_letter_list = edition2_volume_letters[volume_index][volume_letters_index][0]\n",
    "            first_letter_boundary = edition2_volume_letters[volume_index][volume_letters_index][1]\n",
    "        entry_nbr = 0\n",
    "        start_index = line.find(INDEX_STRING)\n",
    "        index = index_string_to_list(line[start_index + len(INDEX_STRING):]) #Identical for lines on same page\n",
    "        index = sorted(index, key=len, reverse=True) # To solve problem (Arm, Armadillo)\n",
    "        print(f\"page_nbr: {page_nbr}: \", index)\n",
    "    else:\n",
    "        line = line.rstrip()[:MAX_ENTRY_LENGTH] # :200\n",
    "        if line:\n",
    "            # --- BOLD MATCHING ---\n",
    "            if line.startswith(tuple([f\"<b>{l}\" for l in first_letter_list])):\n",
    "                is_entry = True\n",
    "                headword = \"\"\n",
    "                matches = re.findall(r'<b>(.*?)<\\/b>', line)\n",
    "                bold_hits += 1\n",
    "                if matches:\n",
    "                    headword = re.sub(r'[,.]$', '', matches[0])\n",
    "\n",
    "            elif line and line[0] in first_letter_list and (len(line) > 40 or \" Se \" in line): # Removing special case\n",
    "\n",
    "                # --- INDEX MATCHING ---    \n",
    "                if index and not \"...\" in index[0]: # index \n",
    "                    smallest_dist = INF\n",
    "                    smallest_index = -1\n",
    "                    for i, index_word in enumerate(index):\n",
    "                        temp_line, temp_index = clean_text_and_index(line, index_word)\n",
    "                        if relative_edit_distance(temp_line[:len(temp_index)], temp_index) < INDEX_SEGMENTER_THRESHOLD: \n",
    "                            headword = index_word\n",
    "                            is_entry = True\n",
    "                            index.pop(i)\n",
    "                            index_hits += 1\n",
    "                            print(f\"Line = {line[:20]}, Index_word: {headword}\")\n",
    "                            break\n",
    "                \n",
    "                # --- CLASSIFIER MATCHING ---\n",
    "                # elif line[0] not capital and not current uppslagsbokstav\n",
    "                #else: use index or neural network\n",
    "                \n",
    "                \n",
    "            if is_entry:\n",
    "                item = {\n",
    "                    \"headword\": headword,\n",
    "                    \"entryid\": entryid,\n",
    "                    \"text\": line,\n",
    "                    \"type\": 0,\n",
    "                    \"qid\": \"0\",\n",
    "                    \"first_edition_key\": \"\",\n",
    "                    \"fourth_edition_key\": \"\"\n",
    "                }\n",
    "                data.append(item)\n",
    "                entry_nbr += 1\n",
    "                is_entry = False\n",
    "            else: \n",
    "                print(f\"NOT FOUND FOR: line = {line[:20]}\")\n",
    "\n",
    "        \n",
    "\n",
    "json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "print(f\"Bold hits: {bold_hits}\")\n",
    "print(f\"Index hits: {index_hits}\")\n",
    "print(f\"Classifier hits: {classifier_hits}\")\n",
    "\n",
    "volume.close()\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial test for creating annotated training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9632it [00:01, 6968.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAKE TO JSON\n"
     ]
    }
   ],
   "source": [
    "#take some real data, create regexes that can extract some features\n",
    "\n",
    "#Comma or period immediately after first word\n",
    "# r\"^[\\p{L}'\\-]+(?=,|\\.)\"\n",
    "\n",
    "#[ character within first 40 characters\n",
    "# r\"^.{0,40}\\[\"\n",
    "\n",
    "#[ character within first 40 characters, comma or period after ]\n",
    "# r\"^.{0,40}\\[.{1,20}?\\](?=\\,|\\.)\"\n",
    "\n",
    "#( character within first 40 characters\n",
    "# r\"^.{0,40}\\(\"\n",
    "\n",
    "#( character within first 40 characters, comma or period after )\n",
    "# r\"^.{0,40}\\(.{1,20}?\\)(?=\\,|\\.)\"\n",
    "\n",
    "#Category word (mus. , bygnk. , kem. ) after first comma or period\n",
    "# r\"^.{1,70}, \\p{L}{1,11}\\.\"\n",
    "\n",
    "#Match first sentence and: . (Capital letter)\n",
    "# r\"^(.*?)\\.\\s\\p{Lu}\"\n",
    "\n",
    "def extract_feature_with_regex(line:str, regex: str) -> int:\n",
    "    return 1 if re.search(regex, line) else 0\n",
    "\n",
    "# Returns a list of sentences in the line\n",
    "# Used to create negative classifications\n",
    "# Add \". X\" to line in function call to get last sentence\n",
    "def extract_sentences_from_line(line:str, sentences: list) -> list[str]:\n",
    "    match = re.findall(r'^(.*?\\.\\s)\\p{Lu}', line)\n",
    "    if not match:\n",
    "        return sentences\n",
    "    text =  re.findall(r'^(.*?\\.\\s)\\p{Lu}', line)[0]\n",
    "    sentences.append(text)\n",
    "    return extract_sentences_from_line(line[len(text):], sentences)\n",
    "\n",
    "classifier_remove_tags = [\n",
    "    [\"<b>\", \"\"],\n",
    "    [\"</b>\", \"\"],\n",
    "]\n",
    "\n",
    "#volume = open(folder_edition2 + \"bo.txt\", \"r\", encoding='utf-8')\n",
    "volumes = [\n",
    "    open(folder_edition2 + \"bo.txt\", \"r\", encoding='utf-8')\n",
    "]\n",
    "\n",
    "labeled_data = []\n",
    "first_letter_list = [\"K\", \"L\"]\n",
    "\n",
    "# i = 1\n",
    "page_nbr = 143\n",
    "is_entry = False\n",
    "for volume in volumes:\n",
    "    for line in tqdm(volume):\n",
    "        # if i > 1900:\n",
    "        #     break\n",
    "        # if i > 1750:\n",
    "        pagenbr_matches = re.search(r'page_number=(\\d+)', line)\n",
    "        if pagenbr_matches:\n",
    "            page_nbr = int(pagenbr_matches.group(1))\n",
    "            # print(f\"page_nbr: {page_nbr}: \")\n",
    "        else:\n",
    "            line = line.rstrip()[:MAX_ENTRY_LENGTH]\n",
    "            if line:\n",
    "                item = {}\n",
    "                # --- BOLD MATCHING --- create ground truth\n",
    "                if line.startswith(\"<b>\"):\n",
    "                    line = clean_html_markup(line, classifier_remove_tags)     \n",
    "                    item[\"class\"] = 1\n",
    "                    is_entry = True\n",
    "\n",
    "\n",
    "                elif line and line[0] in first_letter_list and (len(line) > 40 or \" Se \" in line):\n",
    "                    item[\"class\"] = 0\n",
    "                    is_entry = True\n",
    "                \n",
    "                if is_entry:\n",
    "                    # regexes\n",
    "                    item[\"punctuation_after_first_word\"] = extract_feature_with_regex(line, r\"^[\\p{L}'\\-]+(?=,|\\.)\")\n",
    "                    item[\"square_bracket\"] = extract_feature_with_regex(line, r\"^.{0,40}\\[\")\n",
    "                    item[\"square_bracket_with_punctuation\"] = extract_feature_with_regex(line, r\"^.{0,40}\\[.{1,20}?\\](?=\\,|\\.)\")\n",
    "                    item[\"parentheses\"] = extract_feature_with_regex(line, r\"^.{0,40}\\(\")\n",
    "                    item[\"parentheses_with_punctuation\"] = extract_feature_with_regex(line, r\"^.{0,40}\\(.{1,20}?\\)(?=\\,|\\.)\")\n",
    "                    item[\"category_word\"] = extract_feature_with_regex(line, r\"^.{1,70},\\s+\\p{L}{1,11}\\.\")\n",
    "                    # item[\"Se_keyword\"] = \n",
    "                    \n",
    "                    item[\"text\"] = line #this one should be last\n",
    "\n",
    "                    labeled_data.append(item)\n",
    "                    is_entry = False\n",
    "\n",
    "                line = re.sub(r'^(.*?\\.\\s)\\p{Lu}', '', line)\n",
    "                for sentence in extract_sentences_from_line(line + \". X\", []):\n",
    "                    other_item = {}\n",
    "                    other_item[\"class\"] = 0\n",
    "                    other_item[\"punctuation_after_first_word\"] = extract_feature_with_regex(sentence, r\"^[\\p{L}'\\-]+(?=,|\\.)\")\n",
    "                    other_item[\"square_bracket\"] = extract_feature_with_regex(sentence, r\"^.{0,40}\\[\")\n",
    "                    other_item[\"square_bracket_with_punctuation\"] = extract_feature_with_regex(sentence, r\"^.{0,40}\\[.{1,20}?\\](?=\\,|\\.)\")\n",
    "                    other_item[\"parentheses\"] = extract_feature_with_regex(sentence, r\"^.{0,40}\\(\")\n",
    "                    other_item[\"parentheses_with_punctuation\"] = extract_feature_with_regex(sentence, r\"^.{0,40}\\(.{1,20}?\\)(?=\\,|\\.)\")\n",
    "                    other_item[\"category_word\"] = extract_feature_with_regex(sentence, r\"^.{1,70},\\s+\\p{L}{1,11}\\.\")\n",
    "                    other_item[\"text\"] = sentence\n",
    "                    labeled_data.append(other_item)\n",
    "                    \n",
    "            \n",
    "        # i += 1\n",
    "\n",
    "    volume.close()\n",
    "\n",
    "with open('training_data.json', 'w', encoding='utf-8') as outfile:\n",
    "    print(\"MAKE TO JSON\")\n",
    "    json.dump(labeled_data, outfile, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
