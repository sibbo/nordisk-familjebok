{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation of the text files\n",
    "Segmentiation consists of 3 steps in sequential order. A line is marked as an entry if one of the following holds:\n",
    "1. Line begins with a bold delimiter \\<b>, followed by the current look-up letter.\n",
    "2. A substring from the start of the line matches an index in the index list.\n",
    "3. The logistic regression classification model classifies the line as an entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_helpers as jh\n",
    "import joblib\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import mlp_classifier_utils as mcu # i hate marvel cinematic universe\n",
    "import regex_utils as ru\n",
    "from tqdm.notebook import tqdm\n",
    "from scraping_and_segmenting_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_SEGMENTER_THRESHOLD = 0.15 #relative edit distance threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for entry segmentation using the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_string_to_list(index: str) -> list[str]:\n",
    "    return [query.strip() for query in index.split(\" - \")][1:]\n",
    "\n",
    "def clean_text_and_index(text_word: str, index_word: str) -> str:\n",
    "    # Clean text_word, e.g., remove italic tags, [...].\n",
    "    tags = [\n",
    "                [\"<b>\", \"\"],\n",
    "                [\"</b>\", \"\"],\n",
    "                [\"<i>\", \"\"],\n",
    "                [\"</i>\", \"\"],\n",
    "                ]\n",
    "    text_word = clean_html_markup(text_word, tags)\n",
    "\n",
    "    #if not '[' in index_word:\n",
    "    text_word = re.sub(r'\\s*\\[(.*?)\\]', '', text_word)\n",
    "    index_word = re.sub(r'\\s*\\[(.*?)\\]', '', index_word)    \n",
    "    #if not '(' in index_word:\n",
    "    text_word = re.sub(r'\\s*\\((.*?)\\)', '', text_word)\n",
    "    index_word = re.sub(r'\\s*\\((.*?)\\)', '', index_word)\n",
    "\n",
    "    return text_word, index_word\n",
    "\n",
    "def edit_distance(text_word: str, index_word: str) -> int:\n",
    "    \n",
    "    #Initializing distance matrix\n",
    "    distances = np.zeros((len(text_word) + 1, len(index_word) + 1))\n",
    "    for t1 in range(len(text_word) + 1):\n",
    "        distances[t1][0] = t1\n",
    "    for t2 in range(len(index_word) + 1):\n",
    "        distances[0][t2] = t2\n",
    "\n",
    "    # Computation\n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    \n",
    "    for t1 in range(1, len(text_word) + 1):\n",
    "        for t2 in range(1, len(index_word) + 1):\n",
    "            if (text_word[t1-1] == index_word[t2-1]):\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "            else:\n",
    "                a = distances[t1][t2 - 1]\n",
    "                b = distances[t1 - 1][t2]\n",
    "                c = distances[t1 - 1][t2 - 1]\n",
    "                \n",
    "                if (a <= b and a <= c):\n",
    "                    distances[t1][t2] = a + 1\n",
    "                elif (b <= a and b <= c):\n",
    "                    distances[t1][t2] = b + 1\n",
    "                else:\n",
    "                    distances[t1][t2] = c + 1\n",
    "\n",
    "    return distances[len(text_word)][len(index_word)]\n",
    "\n",
    "def print_distances(distances, token1_length, token2_length):\n",
    "    for t1 in range(token1_length + 1):\n",
    "        for t2 in range(token2_length + 1):\n",
    "            print(int(distances[t1][t2]), end=\" \")\n",
    "        print()\n",
    "\n",
    "def relative_edit_distance(text_word: str, index_word: str) -> float:\n",
    "    return edit_distance(text_word, index_word) / len(index_word)\n",
    "\n",
    "def segment(folder: str, volumes: list[str], edition_nbr: int):\n",
    "    # Load the pre-trained logistic regression model from disk\n",
    "    model = joblib.load('mlp_model.pkl')\n",
    "\n",
    "    \n",
    "    if edition_nbr == 1:\n",
    "        volume_letters = edition1_volume_letters\n",
    "    else:\n",
    "        volume_letters = edition2_volume_letters\n",
    "\n",
    "    data = []\n",
    "    entry_nbr = 0\n",
    "    page_entry_nbr = 0\n",
    "    index = []\n",
    "    is_entry = False\n",
    "    bold_hits = 0\n",
    "    index_hits = 0\n",
    "    classifier_hits = 0\n",
    "    first_letter_list: list[str] = []\n",
    "    classifier_type = 0 # 0 = bold, 1 = index, 2 = neural network/logistic regression classifier\n",
    "    for volume in tqdm(volumes):\n",
    "        first_letter_boundary = 0\n",
    "        volume_letters_index = -1\n",
    "        page_nbr = 0\n",
    "        with open(folder + f\"{volume}.txt\", \"r\", encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                entryid = f\"e{edition_nbr}_{entry_nbr}_{volume}_{page_nbr}_{page_entry_nbr}\"\n",
    "                pagenbr_matches = re.search(r'page_number=(\\d+)', line)\n",
    "                if pagenbr_matches:\n",
    "                    page_nbr = int(pagenbr_matches.group(1))\n",
    "                    if page_nbr > first_letter_boundary:\n",
    "                        volume_letters_index += 1\n",
    "                        first_letter_list = volume_letters[volume][volume_letters_index][0]\n",
    "                        first_letter_boundary = volume_letters[volume][volume_letters_index][1]\n",
    "                    page_entry_nbr = 0\n",
    "                    start_index = line.find(INDEX_STRING)\n",
    "                    index = index_string_to_list(line[start_index + len(INDEX_STRING):]) #Identical for lines on same page\n",
    "                    index = sorted(index, key=len, reverse=True) # To solve problem (Arm, Armadillo)\n",
    "                    # print(f\"page_nbr: {page_nbr}: \", index)\n",
    "                else:\n",
    "                    line = line.rstrip()[:MAX_ENTRY_LENGTH] # :200\n",
    "                    if line and not \"Ord, som saknas under\" in line:\n",
    "                        # --- BOLD MATCHING ---\n",
    "                        if line.startswith(tuple([f\"<b>{l}\" for l in ALPHABET])): #tuple([f\"<b>{l}\" for l in first_letter_list])):\n",
    "                            is_entry = True\n",
    "                            classifier_type = 0\n",
    "                            headword = \"\"\n",
    "                            matches = re.findall(r'<b>(.*?)<\\/b>', line)\n",
    "                            bold_hits += 1\n",
    "                            if matches:\n",
    "                                headword = re.sub(r'[,.]$', '', matches[0])\n",
    "                            else: \n",
    "                                headword = ru.get_headword_no_closing_bold_tag(line)\n",
    "\n",
    "                        elif line[0] in first_letter_list and (len(line) > 50 or \" Se \" in line): # and (len(line) > 40) and not (len(line) < 75 and line.find(\". Se \") != -1): # Removing special case\n",
    "\n",
    "                            # --- INDEX MATCHING ---    \n",
    "                            if index and not any(\"...\" in s for s in index): # index \n",
    "                                for i, index_word in enumerate(index):\n",
    "                                    temp_line, temp_index = clean_text_and_index(line, index_word)\n",
    "                                    if relative_edit_distance(temp_line[:len(temp_index)], temp_index) < INDEX_SEGMENTER_THRESHOLD: \n",
    "                                        headword = ru.get_headword_from_index(index_word)\n",
    "                                        is_entry = True\n",
    "                                        classifier_type = 1\n",
    "                                        index.pop(i)\n",
    "                                        index_hits += 1\n",
    "                                        # print(f\"Line = {line[:20]}, Index_word: {headword}\")\n",
    "                                        break\n",
    "                                if not is_entry:\n",
    "                                    # print(f\"NOT FOUND IN INDEX: {line[:20]}\")\n",
    "                                    pass\n",
    "                            \n",
    "                            # --- CLASSIFIER MATCHING ---\n",
    "                            else: \n",
    "                                x = mcu.transform_sentence(line)\n",
    "                                if model.predict(x)[0] == 1:\n",
    "                                    is_entry = True\n",
    "                                    classifier_type = 2\n",
    "                                    classifier_hits += 1\n",
    "                                    headword = ru.get_headword_from_text(line)\n",
    "                                # else:\n",
    "                                    # print(f\"NON-ENTRY ACCORDING TO CLASSIFIER: {line[:20]}\")\n",
    "\n",
    "                            \n",
    "                        if is_entry:\n",
    "                            item = {\n",
    "                                \"headword\": headword,\n",
    "                                \"entryid\": entryid,\n",
    "                                \"text\": line,\n",
    "                                \"classifier_type\": classifier_type,\n",
    "                                \"class\": 0,\n",
    "                                \"qid\": \"0\",\n",
    "                                \"e2_key\": \"\",\n",
    "                                \"e4_key\": \"\",\n",
    "                                # \"is_cross_ref\": 0,\n",
    "                                \"cross_ref_key\": \"\",\n",
    "                                \"latitude\": None,\n",
    "                                \"longitude\": None,\n",
    "                            }\n",
    "                            data.append(item)\n",
    "                            page_entry_nbr += 1\n",
    "                            entry_nbr += 1\n",
    "                            is_entry = False\n",
    "\n",
    "    jh.write_items(data, f\"{ENCYCLOPEDIAS_JSONS_FOLDER}e{edition_nbr}\")\n",
    "\n",
    "    print(f\"Edition {edition_nbr} stats\\n------------\")\n",
    "    print(f\"Bold hits: {bold_hits}\")\n",
    "    print(f\"Index hits: {index_hits}\")\n",
    "    print(f\"Classifier hits: {classifier_hits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation of the text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment(folder_edition1, edition1_volumes, 1)\n",
    "segment(folder_edition2, edition2_volumes, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
