{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking articles between editions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_searcher import NeuralSearcher\n",
    "from qdrant_client import QdrantClient\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import random\n",
    "\n",
    "e1 = 'e1'\n",
    "e2 = 'e2'\n",
    "MATCH_THRESHOLD = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Qdrant client\n",
    "client = QdrantClient(host=\"localhost\", port=6333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To track progress\n",
    "total_entries = client.count(e1).count\n",
    "\n",
    "# Fetching entries\n",
    "vectors = []\n",
    "batch_size = 50 # This value has to be chosen carefully (Experience from testing)\n",
    "offset = None\n",
    "\n",
    "with tqdm(total=total_entries, desc=\"Fetching entries\") as pbar:\n",
    "    while(True):\n",
    "        response = client.scroll(\n",
    "            collection_name=e1, \n",
    "            with_payload=True, \n",
    "            with_vectors=True, \n",
    "            limit=batch_size,\n",
    "            offset=offset\n",
    "            )\n",
    "        records = response[0]\n",
    "        offset = response[1]\n",
    "        vectors += records\n",
    "        pbar.update(len(records))  # Update progress bar\n",
    "        if len(records) < batch_size:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_searcher_e2 = NeuralSearcher(collection_name=e2)\n",
    "\n",
    "# Create links from entry ids of e1 to e2\n",
    "# And create training data (This is on)\n",
    "text_links = []\n",
    "links_e1_to_e2 = {}\n",
    "for entry in tqdm(vectors, desc=\"Linking entries\"):\n",
    "    entry_id = entry.payload['entryid']\n",
    "    text = entry.payload['text']\n",
    "    matches = neural_searcher_e2.vector_search(entry.vector, threshold=MATCH_THRESHOLD) # Can search with treshold instead\n",
    "    # Run matches through a NN perhaps\n",
    "    if matches:\n",
    "        links_e1_to_e2[entry_id] = matches[0]['entryid'] # Changing the search limit to 1 would make it a lot faster\n",
    "        text_links.append((text, matches[0]['text']))\n",
    "# Create links from entry ids of e2 to e1\n",
    "links_e2_to_e1 = {value: key for key, value in links_e1_to_e2.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make test data for deciding neural search threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_limit = 200\n",
    "data = []\n",
    "samples = random.sample(text_links, data_limit)\n",
    "\n",
    "for sample in samples:\n",
    "    item = {\n",
    "        \"e1_text\": sample[0],\n",
    "        \"e2_text\": sample[1],\n",
    "        \"valid_match\": 1\n",
    "    }\n",
    "    data.append(item)\n",
    "    \n",
    "with open('text_links.json', 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(data, outfile, ensure_ascii=False, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test threshold against test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_links.json', 'r', encoding='utf-8') as test_file:\n",
    "    json_items = json.loads(test_file.read())\n",
    "\n",
    "test_threshold = 0.8\n",
    "nr_matches = 0\n",
    "nr_false_positives = 0\n",
    "nr_false_negatives = 0\n",
    "for item in tqdm(json_items, desc='Verifying matches'):\n",
    "    e1_text = item['e1_text']\n",
    "    e2_text = item['e2_text']\n",
    "    match = neural_searcher_e2.string_search(item['e1_text'], threshold=test_threshold) # String search\n",
    "    if match:\n",
    "        nr_matches += 1\n",
    "        if item['valid_match'] == 0:\n",
    "            nr_false_positives = 0\n",
    "            print(f'Expected no match: \\\"{e1_text}\\\" : \\\"{e2_text}\\\"')\n",
    "    else: \n",
    "        if item['valid_match'] == 1:\n",
    "            nr_false_negatives += 1\n",
    "            print(f'Expected match: \\\"{e1_text}\\\" : \\\"{e2_text}\\\"')\n",
    "\n",
    "print(f\"Made {nr_matches} of {len(json_items)} entries\")\n",
    "print(f\"Number of false positives: {nr_false_positives}\")\n",
    "print(f\"Number of false negatives: {nr_false_negatives}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make linked json files\n",
    "def write_linked_json(in_name: str, out_name: str, links_dict: dict[str, str]) -> None:\n",
    "    with open(in_name, 'r', encoding='utf-8') as infile:\n",
    "        json_items = json.loads(infile.read())\n",
    "    \n",
    "    data = []\n",
    "    for item in json_items:\n",
    "        item['second_edition_key'] = links_dict.get(item['entryid'], \"\")\n",
    "        data.append(item)\n",
    "        \n",
    "    with open(out_name, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_linked_json('e1.json', 'e1_linked.json', links_e1_to_e2)\n",
    "print(\"Finished writing e1_linked.json\")\n",
    "write_linked_json('e2.json', 'e2_linked.json', links_e2_to_e1)\n",
    "print(\"Finished writing e2_linked.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Go through e1\n",
    "#   for every article, search for c\n",
    "\n",
    "# for each article in e2, compare text to qdrant e1, get closest matches\n",
    "# For the closest match, calculate cosine similarity, compare headword, edit-distance, \n",
    "# other features, threshold function.\n",
    "# If match, change in e1 and e2, other edition key to match"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
