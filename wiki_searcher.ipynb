{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for articles in WikiData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json_helpers as jh\n",
    "import regex as re\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from tqdm.notebook import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "WIKIDATA_URL = \"https://www.wikidata.org/w/api.php\"\n",
    "WIKIPEDIA_URL = \"https://sv.wikipedia.org/w/api.php\"\n",
    "model = SentenceTransformer('KBLab/sentence-bert-swedish-cased', device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_wikidata(query: str):\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"format\": \"json\",\n",
    "        \"language\": \"sv\",  # Language dictating how searches are made\n",
    "        \"uselang\": \"sv\",   # Language of item description\n",
    "        \"limit\": 10,       # Number of search results\n",
    "        \"search\": query,\n",
    "    }\n",
    "    response = requests.get(WIKIDATA_URL, params=params)\n",
    "    if 'search' not in response.json().keys():\n",
    "        return None\n",
    "    return response.json()['search']\n",
    "\n",
    "def get_wikipedia_title_from_qid(qid: str):\n",
    "    params = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"format\": \"json\",\n",
    "        # \"lang\": \"sv\",  # Language dictating how searches are made\n",
    "        \"props\": \"sitelinks\",\n",
    "        \"ids\": qid,\n",
    "        \"sitefilter\": \"svwiki\",\n",
    "        \"languages\": \"se\",\n",
    "        # \"uselang\": \"sv\",   # Language of item description\n",
    "        # \"limit\": 10,       # Number of search results\n",
    "        # \"search\": query,\n",
    "    }\n",
    "    response = requests.get(WIKIDATA_URL, params=params)\n",
    "    entity_data = response.json()['entities'].get(qid)\n",
    "\n",
    "    if not entity_data:\n",
    "        print(\"No data found.\")\n",
    "        return None\n",
    "\n",
    "    wikipedia_dict = entity_data['sitelinks'].get('svwiki')\n",
    "\n",
    "    if not wikipedia_dict:\n",
    "        print(\"No svwiki found\")\n",
    "        return None\n",
    "    \n",
    "    return wikipedia_dict['title']\n",
    "    #wikipedia_page_url = f\"https://sv.wikipedia.org/wiki/{wikipedia_dict['title'].replace(' ', '_')}\"\n",
    "    #return wikipedia_page_url\n",
    "\n",
    "def get_first_paragraph_text_wikipedia(qid: str):\n",
    "    # Extract the page title from the Wikipedia link\n",
    "    page_title = get_wikipedia_title_from_qid(qid) # TITLE \n",
    "\n",
    "    # Step 1: Get the Wikipedia page content\n",
    "    page_params = {\n",
    "        \"action\": \"parse\",\n",
    "        \"format\": \"json\",\n",
    "        \"page\": page_title,\n",
    "        \"prop\": \"text\",\n",
    "    }\n",
    "    page_response = requests.get(WIKIPEDIA_URL, params=page_params)\n",
    "    page_data = page_response.json()\n",
    "\n",
    "    if 'error' in page_data:\n",
    "        print(\"Error:\", page_data['error']['info'])\n",
    "        return None\n",
    "\n",
    "    # Step 2: Extract the text of the first paragraph\n",
    "    page_text = page_data['parse']['text']['*']\n",
    "\n",
    "    # Use regex to find the first paragraph within the HTML content\n",
    "    first_paragraph_match = re.search(r'<p>(.*?)</p>', page_text, re.DOTALL)\n",
    "\n",
    "    if first_paragraph_match:\n",
    "        # Remove HTML tags from the paragraph\n",
    "        first_paragraph = re.sub(r'<.*?>', '', first_paragraph_match.group(1))\n",
    "        first_paragraph = re.sub(r'\\[.*?\\]', '', first_paragraph)\n",
    "        return first_paragraph.strip()\n",
    "    else:\n",
    "        print(\"First paragraph not found.\")\n",
    "        return None\n",
    "\n",
    "def search_property(qid: str, prop: str ='P625'):\n",
    "    params = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"format\": \"json\",\n",
    "        \"languages\": \"se\",\n",
    "        \"ids\": qid,\n",
    "        \"props\": \"claims\",\n",
    "    }\n",
    "    response = requests.get(WIKIDATA_URL, params=params)\n",
    "    data = response.json()['entities'].get(qid)\n",
    "    if not data:\n",
    "        print(f\"QID: {qid} was not found\")\n",
    "        return None\n",
    "\n",
    "    prop_claim = data['claims'].get(prop)\n",
    "    if not prop_claim:\n",
    "        print(f\"Entity: {qid} does not have property: {prop}\")\n",
    "        return None\n",
    "\n",
    "    prop_value = prop_claim[0]['mainsnak']['datavalue']['value']\n",
    "    return prop_value\n",
    "\n",
    "def get_qid(entity):\n",
    "    return entity['id']\n",
    "\n",
    "def get_description(entity):\n",
    "    return entity.get('display', '').get('description', '').get('value', '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with wikidata description for comparison of cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the description of each item, compute embedding with kb-sbert\n",
    "# where should we take the data from? qdrant or json or what\n",
    "def search(edition: str):\n",
    "    if edition == 'e1':\n",
    "        items = jh.read_items('e1_linked')\n",
    "    else:\n",
    "        items = jh.read_items('e2_linked')\n",
    "\n",
    "    iterations = 0\n",
    "    for e in items:\n",
    "        iterations += 1\n",
    "        if iterations > 20:\n",
    "            break\n",
    "        search_term = e['headword']\n",
    "        if search_term == \"\":\n",
    "            continue\n",
    "        result = search_wikidata(search_term)\n",
    "        if not result :\n",
    "            continue\n",
    "        vectors = []\n",
    "        for item in tqdm(result):\n",
    "            if \"description\" in item.keys():\n",
    "                vectors.append(model.encode(item[\"description\"]).tolist())\n",
    "            else:\n",
    "                vectors.append([0] * 768)\n",
    "        \n",
    "        example_vector = model.encode(e['text'])\n",
    "        scores = cosine_similarity([example_vector], vectors)[0]\n",
    "\n",
    "        best_match_index = list(scores).index(max(scores))\n",
    "\n",
    "        qid = get_qid(result[best_match_index])\n",
    "        coords = search_property(qid)\n",
    "        if coords == None:\n",
    "            print(f\"{e['headword']}: None\")\n",
    "\n",
    "            e['latitude'] = None\n",
    "            e['longitude'] = None\n",
    "        else:\n",
    "            print(f\"{e['headword']}: {coords['latitude']}, {coords['longitude']}\")\n",
    "\n",
    "            e['type'] = 1\n",
    "            e['latitude'] = coords['latitude']\n",
    "            e['longitude'] = coords['longitude']\n",
    "        \n",
    "        e['qid'] = qid\n",
    "        \n",
    "    jh.write_items(items, f'{edition}_linked2')\n",
    "\n",
    "    # results\n",
    "\n",
    "    # return the one with the highest cosine sim\n",
    "    # if it has P625 property (coordinate location), edit something\n",
    "    # edit json or smth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(edition='e1')\n",
    "search(edition='e2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example_vectors = []\n",
    "for e in examples:\n",
    "    example_vectors.append(model.encode(e))\n",
    "\n",
    "scores = []\n",
    "for e in example_vectors:\n",
    "    cosine_similarity([e], vectors)[0]  # Look for the most similar vectors, manually score all vectors\n",
    "scores\n",
    "best_match_index = list(scores).index(max(scores))\n",
    "\n",
    "result[best_match_index]\n",
    "# top_scores_ids = np.argsort(scores)[-5:][::-1]  # Select top-5 with vectors the largest scores   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
