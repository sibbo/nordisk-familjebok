{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import regex as re\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "INF = 10**9\n",
    "MAX_ENTRY_LENGTH = 200\n",
    "INDEX_SEGMENTER_THRESHOLD = 0.15\n",
    "ENCYCLOPEDIAS_FOLDER = \"encyclopedias/\"\n",
    "PAGE_NUMBER_STRING = \"page_number=\"\n",
    "INDEX_STRING = \"index=\"\n",
    "\n",
    "INDEX_BEGIN = \"<b>On this page / på denna sida</b>\\n\"\n",
    "INDEX_END = \"<p>\"\n",
    "\n",
    "DELIM_BEGIN = \"<!-- mode=normal -->\"\n",
    "DELIM_END = \"<!-- NEWIMAGE2 -->\"\n",
    "\n",
    "html_entities = [\n",
    "                [\"&quot;\", \"\\\"\"],\n",
    "                [\"&rsquo;\", \"\\'\"],\n",
    "                [\"&lsquo;\", \"\\'\"],\n",
    "                [\"&ndash;\", \"-\"],\n",
    "                [\"<br>\", \"\"],\n",
    "                ['<span class=\"sp\">', \"\"],\n",
    "                [\"</span>\", \"\"],\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_substring_between_delimiters(s: str, start: str, end: str):\n",
    "    start_index = s.find(start)\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "    start_index += len(start)\n",
    "    end_index = s.find(end, start_index)\n",
    "    if end_index == -1:\n",
    "        return None\n",
    "\n",
    "    return s[start_index: end_index]\n",
    "\n",
    "def clean(s: str, tag: str, new: str = \"\"):\n",
    "    return s.replace(tag, new)\n",
    "\n",
    "def remove_single_newline(s: str):\n",
    "    return re.sub(r'(?<!\\n)\\n(?!\\n)|(\\n+)(?=\\n)', ' ', s)\n",
    "\n",
    "def scrape_page(url: str) -> str:\n",
    "    try:\n",
    "        page = urlopen(url)\n",
    "    except:\n",
    "        return None\n",
    "    html = page.read().decode(\"utf-8\")\n",
    "    index = get_substring_between_delimiters(html, INDEX_BEGIN, INDEX_END)\n",
    "    html = get_substring_between_delimiters(html, DELIM_BEGIN, DELIM_END)\n",
    "    if not index == None:\n",
    "        for pair in html_entities:\n",
    "            index = clean(index, pair[0], pair[1])\n",
    "        index = remove_single_newline(index)\n",
    "    if not html == None:\n",
    "        for pair in html_entities:\n",
    "            html = clean(html, pair[0], pair[1])\n",
    "        html = remove_single_newline(html)\n",
    "    # print(html)\n",
    "    return html, index\n",
    "\n",
    "def create_url(partial_url: str, i: int):\n",
    "    return partial_url + f\"{i:04d}\" + \".html\"\n",
    "\n",
    "def scrape_volume(base_url: str, volume_start_number: int, volume_end_number: int = 9999999):\n",
    "    i = volume_start_number\n",
    "    volume_str: str = \"\"\n",
    "    while(True):\n",
    "        url = create_url(base_url, i)\n",
    "        text, index = scrape_page(url)\n",
    "        if text == None or index == None:\n",
    "            break\n",
    "        volume_str += PAGE_NUMBER_STRING + str(i) + \", \"\n",
    "        volume_str += INDEX_STRING + index + \"\\n\"\n",
    "        volume_str += text\n",
    "        print(f\"i = {i}: {volume_str[-10:]}\")\n",
    "        i += 1\n",
    "        if i > volume_end_number:\n",
    "            break\n",
    "    return volume_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the second edition (ugglan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://runeberg.org/nf\"\n",
    "# base_url = \"http://runeberg.org/download.pl?mode=ocrtext&work=nf\"\n",
    "\n",
    "#the ranges for the urls, they have a slightly weird format\n",
    "uggla_url_range = {\n",
    "    'b': \"abcdefghijklmnopqrst\",\n",
    "    'c': \"abcdefghijklmn\",\n",
    "}\n",
    "\n",
    "#first two volumes start on this number\n",
    "volume_start_number_ba_bb = 795 #13\n",
    "\n",
    "#the rest start on this one\n",
    "volume_start_number = 192 #17\n",
    "\n",
    "#folder to save the .txt files in\n",
    "folder = ENCYCLOPEDIAS_FOLDER + \"second/\"\n",
    "\n",
    "# main loop\n",
    "# for first_letter in ('b', 'c'):\n",
    "#     for second_letter in uggla_url_range[first_letter]:\n",
    "#         volume_index = first_letter + second_letter\n",
    "#         f = open(folder + volume_index + \".txt\", \"w\")\n",
    "#         idx_file = open(folder + volume_index + \"_idx.txt\")\n",
    "#         print(f\"volume index: {volume_index}\")\n",
    "#         volume_url = base_url + volume_index + \"/\"\n",
    "#         print(volume_url)\n",
    "#         if volume_index in [\"ba\", \"bb\"]:\n",
    "#             f.write(scrape_volume(volume_url, volume_start_number_ba_bb)) #här ska det läggas in indexgrejer också\n",
    "#         else:\n",
    "#             f.write(scrape_volume(volume_url, volume_start_number))\n",
    "#         f.close()\n",
    "#         idx_file.close()\n",
    "\n",
    "volume_index = 'ba'\n",
    "f = open(folder + volume_index + \".txt\", \"w\", encoding='utf-8')\n",
    "print(f\"volume index: {volume_index}\")\n",
    "volume_url = base_url + volume_index + \"/\"\n",
    "print(volume_url)\n",
    "if volume_index in [\"ba\", \"bb\"]:\n",
    "    text = scrape_volume(volume_url, volume_start_number_ba_bb)\n",
    "    print(text)\n",
    "    f.write(text)\n",
    "else:\n",
    "    f.write(scrape_volume(volume_url, volume_start_number, volume_end_number=200))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_index(index: str):\n",
    "    return [query.strip() for query in index.split(\" - \")][1:]\n",
    "\n",
    "def pre_dist_clean(text_word: str, index_word: str) -> str:\n",
    "    # Clean text_word, e.g., remove italic tags, [...].\n",
    "    tags = [\n",
    "                [\"<b>\", \"\"],\n",
    "                [\"</b>\", \"\"],\n",
    "                [\"<i>\", \"\"],\n",
    "                [\"</i>\", \"\"],\n",
    "                ]\n",
    "    for pair in tags:\n",
    "        text_word = clean(text_word, pair[0], pair[1])\n",
    "\n",
    "    #if not '[' in index_word:\n",
    "    text_word = re.sub(r'\\s*\\[(.*?)\\]', '', text_word)\n",
    "    index_word = re.sub(r'\\s*\\[(.*?)\\]', '', index_word)    \n",
    "    #if not '(' in index_word:\n",
    "    text_word = re.sub(r'\\s*\\((.*?)\\)', '', text_word)\n",
    "    index_word = re.sub(r'\\s*\\((.*?)\\)', '', index_word)\n",
    "\n",
    "    return text_word, index_word\n",
    "\n",
    "def edit_distance(text_word: str, index_word: str) -> int:\n",
    "    \n",
    "    #Initializing distance matrix\n",
    "    distances = np.zeros((len(text_word) + 1, len(index_word) + 1))\n",
    "    for t1 in range(len(text_word) + 1):\n",
    "        distances[t1][0] = t1\n",
    "    for t2 in range(len(index_word) + 1):\n",
    "        distances[0][t2] = t2\n",
    "\n",
    "    # Computation\n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    \n",
    "    for t1 in range(1, len(text_word) + 1):\n",
    "        for t2 in range(1, len(index_word) + 1):\n",
    "            if (text_word[t1-1] == index_word[t2-1]):\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "            else:\n",
    "                a = distances[t1][t2 - 1]\n",
    "                b = distances[t1 - 1][t2]\n",
    "                c = distances[t1 - 1][t2 - 1]\n",
    "                \n",
    "                if (a <= b and a <= c):\n",
    "                    distances[t1][t2] = a + 1\n",
    "                elif (b <= a and b <= c):\n",
    "                    distances[t1][t2] = b + 1\n",
    "                else:\n",
    "                    distances[t1][t2] = c + 1\n",
    "\n",
    "    return distances[len(text_word)][len(index_word)]\n",
    "\n",
    "def printDistances(distances, token1Length, token2Length):\n",
    "    for t1 in range(token1Length + 1):\n",
    "        for t2 in range(token2Length + 1):\n",
    "            print(int(distances[t1][t2]), end=\" \")\n",
    "        print()\n",
    "\n",
    "def relative_edit_distance(text_word: str, index_word: str) -> float:\n",
    "    return edit_distance(text_word, index_word) / len(index_word)\n",
    "\n",
    "def line_contains_index(line: str, index_word) -> bool:\n",
    "    return edit_distance(line[:len(index_word)], index_word) < 5\n",
    "    \n",
    "# def is_match_head_index(text_word: str, index_word: str) -> bool:\n",
    "#     edit_dist = edit_distance(text_word, index_word)\n",
    "#     return relative_edit_distance(edit_dist, len(index_word)) < 0.1 # Need to test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text som borde matchas men gör det inte\n",
    "\n",
    "# Denna går in att matcha pga, index_word innehåller '(' så då tar vi inte bort \"(...)\"\n",
    "# Den innehåller två sekvenser av \"(...)\" där den sista är med. Lösning: IDK, kanske ta bort från båda, det skulle lösa problemet såvida\n",
    "# det inte finns två index ord där det enda som skiljer är nå parantes. \n",
    "text_word1 = \"<b>Argyroid</b> l. Argyrofan (af grek. <i>argyros</i>, silfver, och <i>eidos</i>, utseende, l. <i>fainesthai</i>, synas), namn på vissa slag af nysilfver (se d. o.). \"\n",
    "index_word1 = \"Argyroid l. Argyrofan, namn på vissa slag af nysilfver (se d. o.)\"\n",
    "temp_line1, temp_index1 = pre_dist_clean(text_word1, index_word1)[:len(index_word1)]\n",
    "print(temp_line1[:len(temp_index1)])\n",
    "print(temp_index1)\n",
    "print(relative_edit_distance(temp_line1[:len(temp_index1)], temp_index1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_word  = \"XXXXXXXX\"\n",
    "index_word = \"XXXXX\"\n",
    "print(relative_edit_distance(text_word, index_word)) # Armadillon\n",
    "\n",
    "#Armadillo\n",
    "# Arm\n",
    "# Arma\n",
    "# 1: Arm - Armadillo , OOPS dessa matchade perfekt för vi gjorde de till samma längd\n",
    "# 2: Arm - Arm, OOPS nu kan vi inte längre matcha dessa.\n",
    "# 3: Den måste föredra matchningar med längre index ord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"a\", \"ac\", \"abc\"]\n",
    "test = sorted(test, key=len, reverse=True)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = open(folder + \"ba.txt\", \"r\", encoding='utf-8')\n",
    "json_file = open(\"nf.json\", 'a', encoding='utf-8')\n",
    "\n",
    "#loopa igenom hela filen, om raden har bold tags så tar vi tills\n",
    "#nästa newline eller de första 200 tecknen, den som kommer först\n",
    "#och sparar till en json-fil\n",
    "\n",
    "#TEMPORARY\n",
    "edition_nbr = 2\n",
    "volume_nbr = 1\n",
    "\n",
    "data = []\n",
    "entry_nbr = 0\n",
    "page_nbr = 0\n",
    "index = []\n",
    "is_entry = False\n",
    "index_hits = 0\n",
    "for line in volume:\n",
    "    entryid = f\"e{edition_nbr}_v{volume_nbr}_{page_nbr}_{entry_nbr}\"\n",
    "    pagenbr_matches = re.search(r'page_number=(\\d+)', line)\n",
    "    if pagenbr_matches:\n",
    "        page_nbr = int(pagenbr_matches.group(1))\n",
    "        if page_nbr == 796: # REMOVE\n",
    "            break           # REMOVE\n",
    "        entry_nbr = 0\n",
    "        start_index = line.find(INDEX_STRING)\n",
    "        index = prep_index(line[start_index + len(INDEX_STRING):]) #Identical for lines on same page\n",
    "        index = sorted(index, key=len, reverse=True) # To solve problem (Arm, Armadillo)\n",
    "        print(f\"page_nbr: {page_nbr}: \", index)\n",
    "    else:\n",
    "        line = line.rstrip()[:MAX_ENTRY_LENGTH] # :200\n",
    "        # if line.startswith(\"<b>\"):\n",
    "        #     is_entry = True\n",
    "        #     headword = \"\"\n",
    "        #     matches = re.findall(r'<b>(.*?)<\\/b>', line)\n",
    "        #     if matches:\n",
    "        #         headword = re.sub(r'[,.]$', '', matches[0])\n",
    "            \n",
    "        if not \"...\" in index[0]: # index \n",
    "            smallest_dist = INF\n",
    "            smallest_index = -1\n",
    "            print(len(index))\n",
    "            for i, index_word in enumerate(index):\n",
    "                temp_line, temp_index = pre_dist_clean(line, index_word)\n",
    "                # dist = (relative_edit_distance(temp_line, index_word))\n",
    "                # if dist < smallest_dist:\n",
    "                #     smallest_dist = dist\n",
    "                #     smallest_index = i\n",
    "                if relative_edit_distance(temp_line[:len(temp_index)], temp_index) < INDEX_SEGMENTER_THRESHOLD: \n",
    "                    smallest_dist = 0\n",
    "                    smallest_index = i\n",
    "                    print(f\"smallest_dist = {smallest_dist}, Index example: line = {line[:20]}, index_word: {index[smallest_index]}\")\n",
    "                    index.pop(smallest_index)\n",
    "                    index_hits += 1\n",
    "                    break\n",
    "            if smallest_index == -1:\n",
    "                print(f\"NOT FOUND FOR: line = {line[:20]}\")\n",
    "                \n",
    "            #threshold\n",
    "            # if smallest_dist < INDEX_SEGMENTER_THRESHOLD:\n",
    "            #     is_entry = True\n",
    "            #     headword = index_word\n",
    "            #     index_hits += 1\n",
    "                \n",
    "        if is_entry:\n",
    "            item = {\n",
    "                \"headword\": headword,\n",
    "                \"entryid\": entryid,\n",
    "                \"text\": line,\n",
    "                \"type\": 0,\n",
    "                \"qid\": \"0\",\n",
    "                \"first_edition_key\": \"\",\n",
    "                \"fourth_edition_key\": \"\"\n",
    "            }\n",
    "            data.append(item)\n",
    "            entry_nbr += 1\n",
    "            is_entry = False\n",
    "        \n",
    "\n",
    "    #else: use index or neural network\n",
    "\n",
    "json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "print(f\"index hits: {index_hits}\")\n",
    "\n",
    "volume.close()\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_edit_distance(\"Argus IV, en \", \"Argus-fjärilen, äfven kallad Allmänna blåvingen, Lycæna argus, zool.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "characters = ['a','b','c','d','f']\n",
    "characters.pop(2)\n",
    "print(characters)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
