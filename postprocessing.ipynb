{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post procesing of the json-files\n",
    "- Remove articles in false order\n",
    "- Linking cross references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from utils.paths import *\n",
    "from utils import json_helpers as jh\n",
    "import regex as re\n",
    "import bisect\n",
    "import random\n",
    "import copy\n",
    "\n",
    "e1 = f'{ENCYCLOPEDIAS_JSON_FOLDER}/e1'\n",
    "e2 = f'{ENCYCLOPEDIAS_JSON_FOLDER}/e2'\n",
    "\n",
    "e1_eval_order = f'{ORDER_TEST_FOLDER}/e1_test_order'\n",
    "e2_eval_order = f'{ORDER_TEST_FOLDER}/e2_test_order'\n",
    "\n",
    "e1_test_links_json = f'{CROSS_TEST_FOLDER}/e1_test_links'\n",
    "e2_test_links_json = f'{CROSS_TEST_FOLDER}/e2_test_links'\n",
    "\n",
    "e1_stats_links_json = f'{CROSS_STATS_FOLDER}/e1_links_recall'\n",
    "e2_stats_links_json = f'{CROSS_STATS_FOLDER}/e2_links_recall'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing articles in false order\n",
    "True articles will are ordered alphabetically in the editions. Outliers that disrupt this order are not articles and must be removed...\n",
    "\n",
    "- Method: Loop through entries. Remove entry based on comparison with previous and next words. Context can be of any odd size but 5 seems to be enough.\n",
    "- It is safer to compare less characters. We found the first 3 to work best.\n",
    "- The encyclopedia orders words by letter but ignores characters which are not letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_CHAR_COMPARE = 3\n",
    "\n",
    "# Removes non-letter characters\n",
    "def replace_uncommon_chars(text: str) -> str:\n",
    "    replacements = [\n",
    "        (r'[^\\p{L}]', ''),\n",
    "        (r'Ü', 'U'),\n",
    "        (r'ü', 'u'),\n",
    "        (r'W', 'V'), # Encyclopedia sees 'w' as 'v'\n",
    "        (r'w', 'v'),\n",
    "        (r'\\'', '')\n",
    "\n",
    "    ]\n",
    "    for p in replacements:\n",
    "        text = re.sub(p[0], p[1], text)\n",
    "    return text\n",
    "\n",
    "def nr_chars_match(word1: str, word2: str) -> int:\n",
    "    shortest_word_len = min(len(word1), len(word2))\n",
    "    nr_matches = 0\n",
    "    for i in range(shortest_word_len):\n",
    "        if word1[i] == word2[i]:\n",
    "            nr_matches += 1\n",
    "    return nr_matches + (abs(len(word1) - len(word2)))\n",
    "\n",
    "# Returns true if word at center of window is to be removed.\n",
    "# Insert parameters are for handling inputs where target word is not in center\n",
    "def remove_condition(words: list[str], insert_back: int=0, insert_front: int=0) -> bool:\n",
    "    window = []\n",
    "    # Insert smallest string in back \n",
    "    for word in range(insert_back):\n",
    "        window.append(\"\") # \"\" is smaller than the smallest letter\n",
    "\n",
    "    # Append words\n",
    "    for word in words:\n",
    "        window.append(replace_uncommon_chars(word).lower()[:NR_CHAR_COMPARE])\n",
    "\n",
    "    # Inserst biggest string in front\n",
    "    for word in range(insert_front):\n",
    "        window.append(\"Ø\") # \"Ø\" is greater than the biggest letter \n",
    "    \n",
    "    test_word = window[len(window) // 2]\n",
    "    back_approved = 0\n",
    "    front_approved = 0\n",
    "    for i in range(0, len(window) // 2):\n",
    "        if test_word >= window[i]:\n",
    "            back_approved += 1\n",
    "        # if nr_chars_match(test_word, window[i]) > NR_CHAR_COMPARE // 2:\n",
    "        #     back_approved += 1\n",
    "    for i in range((len(window) // 2) + 1, len(window)):\n",
    "        if test_word <= window[i]:\n",
    "            front_approved += 1     \n",
    "        # if nr_chars_match(test_word, window[i]) > NR_CHAR_COMPARE // 2:\n",
    "        #     front_approved += 1  \n",
    "    return (back_approved + front_approved) <= (len(window) // 2)\n",
    "\n",
    "def nr_back_front_inserts(idx: int, context_size: int, list_size: int) -> tuple[int, int]:\n",
    "    back_inserts = max(0, context_size - idx)\n",
    "    front_inserts = max(0, context_size - (list_size - 1 - idx))\n",
    "    return back_inserts, front_inserts\n",
    "\n",
    "def remove_unordered(edition_name: str, context_size: int=2):\n",
    "    entries = jh.read_items(edition_name)\n",
    "    entries_ord = []\n",
    "    entries_removed = []\n",
    "    for i in tqdm(list(range(len(entries))), desc=f\"Removing unordered entries\"):\n",
    "        if entries[i]['text'][:3] == \"<b>\":\n",
    "            continue\n",
    "        back_inserts, front_inserts = nr_back_front_inserts(i, context_size, len(entries))\n",
    "        context_words = list(map(lambda a: a.get('headword'), entries[max(0, i - context_size): i + context_size + 1]))\n",
    "        if remove_condition(context_words, back_inserts, front_inserts):\n",
    "            entries_removed.append(copy.deepcopy(entries[i]))\n",
    "        else:\n",
    "            entries_ord.append(entries[i])\n",
    "    \n",
    "    print(f\"Number of entries removed: {len(entries_removed)}\")\n",
    "    return entries_ord, entries_removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1_ordered, e1_removed = remove_unordered(e1)\n",
    "e2_ordered, e2_removed = remove_unordered(e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make evaluation data of removed entries\n",
    "* This data has to be manually annotated as valid removal (1) or not valid (0) removal.\n",
    "* This step can be skipped (go to \"Linking cross references\") if these statistics are not of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data sets for manual annotation\n",
    "e1_removed_data = random.sample(e1_removed, 50)\n",
    "e2_removed_data = random.sample(e2_removed, 50)\n",
    "\n",
    "for entry in e1_removed_data:\n",
    "    entry['valid_removal'] = 0\n",
    "\n",
    "for entry in e2_removed_data:\n",
    "    entry['valid_removal'] = 0\n",
    "\n",
    "jh.write_items(e1_removed_data, e1_eval_order)\n",
    "jh.write_items(e2_removed_data, e2_eval_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking cross references\n",
    "\n",
    "#### Cross reference linking functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cross_ref(text: str):\n",
    "    return len(text) < 60 and \" Se \" in text\n",
    "\n",
    "# Get reference word (what is after 'Se ')\n",
    "def get_ref_word(text: str) -> str:\n",
    "    match = re.search(r'Se\\s+([^.]+)\\.', text)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return \"\"\n",
    "\n",
    "# Find index of headword\n",
    "def binary_search(arr, target):\n",
    "    index = bisect.bisect_left(arr, target)\n",
    "    if index != len(arr) and arr[index] == target:\n",
    "        return index\n",
    "    return -1\n",
    "\n",
    "def get_index(lst: list, item) -> int:\n",
    "    try:\n",
    "        return lst.index(item)\n",
    "    except ValueError:\n",
    "        return -1\n",
    "\n",
    "# Cross reference linking logic\n",
    "def cross_link(entries: list[dict]):\n",
    "    headwords = [entry['headword'] for entry in entries]\n",
    "\n",
    "    nr_linked = 0\n",
    "    # Assign cross references\n",
    "    for entry in tqdm(entries, desc=\"Finding cross references\"):\n",
    "        if is_cross_ref(entry['text']):\n",
    "            ref_word = get_ref_word(entry['text'])\n",
    "            # Binary search probably doesn't work since we can't guarantee that it is ordered\n",
    "            # 2500 articles are in false order but valid articles either way\n",
    "            # idx = binary_search(headwords, ref_word) \n",
    "            idx = get_index(headwords, ref_word)\n",
    "            if idx != -1:\n",
    "                entry['cross_ref_key'] = entries[idx]['entryid']\n",
    "                nr_linked += 1\n",
    "                continue\n",
    "            else:\n",
    "                entry['cross_ref_key'] = \"-\"\n",
    "    print(f\"Number of cross references linked: {nr_linked}\")\n",
    "    return entries, nr_linked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running methods for each edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1_ordered = jh.read_items(e1)\n",
    "e2_ordered = jh.read_items(e2)\n",
    "\n",
    "e1_final, e1_nr_linked = cross_link(e1_ordered)\n",
    "e2_final, e2_nr_linked = cross_link(e2_ordered)\n",
    "\n",
    "jh.write_items(e1_final, e1)\n",
    "jh.write_items(e2_final, e2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make evaluation data of linked entries\n",
    "* This data has to be manually annotated as valid link (1) or not valid (0) link.\n",
    "* This step can be skipped if these statistics are not of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data sets for manual annotation\n",
    "e1_linked_data = random.sample(e1_final, 100)\n",
    "e2_linked_data = random.sample(e2_final, 100)\n",
    "\n",
    "for entry in e1_linked_data:\n",
    "    entry['is_cross_ref'] = 0\n",
    "\n",
    "for entry in e2_linked_data:\n",
    "    entry['is_cross_ref'] = 0\n",
    "\n",
    "jh.write_items(item_e1, e1_stats_links_json)\n",
    "jh.write_items(item_e2, e2_stats_links_json)\n",
    "\n",
    "item_e1 = [{\n",
    "    \"Nr_articles\": len(e1_final),\n",
    "    \"Nr_cross_ref_linked\": e1_nr_linked,\n",
    "}]\n",
    "\n",
    "item_e2 = [{\n",
    "    \"Nr_articles\": len(e2_final),\n",
    "    \"Nr_cross_ref_linked\": e2_nr_linked,\n",
    "}]\n",
    "\n",
    "jh.write_items(e1_linked_data, e1_test_links_json)\n",
    "jh.write_items(e2_linked_data, e2_test_links_json)\n",
    "\n",
    "\n",
    "# jh.write_items(e1_linked_data, e1_test_links_json)\n",
    "# jh.write_items(e2_linked_data, e2_test_links_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
